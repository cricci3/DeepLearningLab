{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Addition using Recurrent Neural Networks\nThis notebook demonstrates how to perform a simple arithmetic operation, specifically addition, using a Recurrent Neural Network (RNN). The goal is to train an RNN model that can take two integers as inputs and produce their sum as output. \n\nAs it is easy to note, this does not work. Can you fix it?","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:31:21.522588Z","iopub.execute_input":"2024-10-28T11:31:21.523072Z","iopub.status.idle":"2024-10-28T11:31:24.924148Z","shell.execute_reply.started":"2024-10-28T11:31:21.523034Z","shell.execute_reply":"2024-10-28T11:31:24.923109Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data\nIn this section, we generate synthetic data for training the RNN. Each data point consists of two integers $ A $ and $ B $, where $ X = (A, B) $ represents the input, and $ Y = A + B $ serves as the target output. \n\nWe define a function `generate_data` to create random integer pairs","metadata":{}},{"cell_type":"code","source":"def generate_data(n_samples, n_min = 1, n_max = 100, seed=42):\n    np.random.seed(seed)\n    X = np.zeros((n_samples, 2), dtype=int)\n    cont = 0\n    while cont < n_samples:\n        A = np.random.randint(n_min, n_max)\n        B = np.random.randint(n_min, n_max)\n        X[cont, 0] = A\n        X[cont, 1] = B\n        cont += 1\n        X[cont, 1] = A\n        X[cont, 0] = B\n        cont += 1\n    return X\n\nn_min = 0\nn_max = 100\nX_int_train = generate_data(10000, n_min = n_min, n_max = n_max)\n# No validation for this simple example\nprint(X_int_train[0:6, :])","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:31:24.926811Z","iopub.execute_input":"2024-10-28T11:31:24.927295Z","iopub.status.idle":"2024-10-28T11:31:24.998871Z","shell.execute_reply.started":"2024-10-28T11:31:24.927247Z","shell.execute_reply":"2024-10-28T11:31:24.997958Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[[51 92]\n [92 51]\n [14 71]\n [71 14]\n [60 20]\n [20 60]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Vocabulary\n\nTo process the input data, we need to map each character or symbol in the input to a numerical representation. We create a vocabulary containing digits and special characters used in this task:\n\n1. **Digits (0-9)**: Each digit is represented by its corresponding index.\n2. **Special Symbols**: The addition symbol \"+\" is mapped to a unique index, a padding token `<PAD>` is introduced to handle sequence lengths, `<EOS>` token denotes the end of the sequence","metadata":{}},{"cell_type":"code","source":"int_to_idx = {str(i) : i for i in range(10)}\nint_to_idx['+'] = 10\nint_to_idx['<PAD>'] = 11\nint_to_idx['<EOS>'] = 12\n\nidx_to_int = {i : str(i) for i in range(10)}\nidx_to_int[10] = '+'\nidx_to_int[11] = \"<PAD>\"\nidx_to_int[12] = \"<EOS>\"","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:31:25.000010Z","iopub.execute_input":"2024-10-28T11:31:25.000312Z","iopub.status.idle":"2024-10-28T11:31:25.005791Z","shell.execute_reply.started":"2024-10-28T11:31:25.000280Z","shell.execute_reply":"2024-10-28T11:31:25.004699Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Dataset and Dataloaders\n\nBefore training, we need to encode the input sequences as numerical arrays, using the vocabulary defined earlier. The model expects fixed-length sequences, so padding is applied to ensure uniform input size.","metadata":{}},{"cell_type":"code","source":"class IntDataset(Dataset):\n    def __init__(self, X, int_to_idx):\n        self.num_data = X\n        self.vocab_size  = len(int_to_idx)\n        self.map = int_to_idx\n\n    def __len__(self):\n        return self.num_data.shape[0]\n\n    def __getitem__(self, idx):\n        '''\n        Input: two integers\n        Output: their sum\n        '''\n        X = []\n        y = []\n        A, B = self.num_data[idx, :]\n        A = str(A)\n        B = str(B)\n        \n        # Take the maximum length \n        length_max = max(len(A), len(B))\n        \n        # Pad both to the maximal lengh\n        while len(A) < length_max:\n            A = '0' + A\n        while len(B) < length_max:\n            B = '0' + B\n            \n        for c in A:\n            X.append(self.map[c])\n        \n        X.append(self.map[\"+\"])\n        \n        \n        for c in B:\n            X.append(self.map[c])\n            \n        X.append(self.map[\"<EOS>\"])\n        \n        C = int(A) + int(B)\n        \n        C = str(C)\n        for c in C:\n            y.append(self.map[c])\n            \n        y.append(self.map[\"<EOS>\"])\n        \n        return torch.tensor(X), torch.tensor(y)\n\n# Dataset for the training\ndataset_train = IntDataset(X_int_train, int_to_idx)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:31:25.008061Z","iopub.execute_input":"2024-10-28T11:31:25.008419Z","iopub.status.idle":"2024-10-28T11:31:25.024480Z","shell.execute_reply.started":"2024-10-28T11:31:25.008379Z","shell.execute_reply":"2024-10-28T11:31:25.023396Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch, pad_value):\n    data, targets = zip(*batch)\n\n    padded_data = nn.utils.rnn.pad_sequence(data, batch_first=True,\n                                          padding_value=pad_value)\n    padded_targets = nn.utils.rnn.pad_sequence(targets, batch_first=True,\n                                             padding_value=pad_value)\n    \n    return padded_data, padded_targets\n\n# Dataloader\nbatch_size = 8\ndataloader_train = DataLoader(dataset_train, batch_size=batch_size, collate_fn=lambda b: collate_fn(b, int_to_idx[\"<PAD>\"]),  shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:31:25.025591Z","iopub.execute_input":"2024-10-28T11:31:25.025899Z","iopub.status.idle":"2024-10-28T11:31:25.042078Z","shell.execute_reply.started":"2024-10-28T11:31:25.025868Z","shell.execute_reply":"2024-10-28T11:31:25.041251Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"In this part, we define the architecture of the RNN. The model consists of:\n\n1. **Embedding Layer**: Transforms the input indices into dense vector representations.\n2. **RNN Layer**: Processes the sequence to capture dependencies between the input values.\n3. **Output Layer**: Maps the RNN's hidden states to a final output.","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, int_to_idx, idx_to_int, hidden_size, emb_dim=8, n_layers=1):\n        super(Model, self).__init__()\n\n        self.vocab_size  = len(int_to_idx)\n        self.hidden_size = hidden_size\n        self.emb_dim     = emb_dim\n        self.n_layers    = n_layers\n\n        self.embedding = nn.Embedding(\n            num_embeddings=self.vocab_size,\n            embedding_dim =self.emb_dim,\n            padding_idx=int_to_idx[\"<PAD>\"])\n\n        self.rnn = nn.RNN(input_size=self.emb_dim,\n                          hidden_size=self.hidden_size,\n                          num_layers =self.n_layers,\n                          batch_first=True, nonlinearity='relu')\n\n\n        self.fc = nn.Linear(\n            in_features =self.hidden_size,\n            out_features=self.vocab_size)\n\n    def forward(self, x, prev_state):\n        embed = self.embedding(x)\n        yhat, state = self.rnn(embed, prev_state)\n        out = self.fc(yhat)\n        return out, state\n\n    def init_state(self, batch_size=2):\n        if batch_size > 1:\n            return torch.zeros(self.n_layers, batch_size, self.hidden_size)\n        else:\n            return torch.zeros(self.n_layers, self.hidden_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:31:25.043377Z","iopub.execute_input":"2024-10-28T11:31:25.043763Z","iopub.status.idle":"2024-10-28T11:31:25.055162Z","shell.execute_reply.started":"2024-10-28T11:31:25.043699Z","shell.execute_reply":"2024-10-28T11:31:25.054220Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"model = Model(int_to_idx, idx_to_int, hidden_size = 128, n_layers=2)\nepochs = 100\nlr = 0.001\ncriterion = nn.CrossEntropyLoss(ignore_index=int_to_idx[\"<PAD>\"])\noptimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = 0.99)\n\n# Set the exact device \nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n    if torch.backends.mps.is_available() else 'cpu')\nmodel = model.to(DEVICE)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:31:25.056458Z","iopub.execute_input":"2024-10-28T11:31:25.056850Z","iopub.status.idle":"2024-10-28T11:31:26.687514Z","shell.execute_reply.started":"2024-10-28T11:31:25.056803Z","shell.execute_reply":"2024-10-28T11:31:26.686681Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model.train()\n\nfor epoch in range(epochs):\n    running_loss = 0\n    for X, y in dataloader_train:\n        # Pad y until the length of X with <PAD> token \n        diff_shape = X.shape[1] - y.shape[1]\n        \n        y_pad = torch.full((y.shape[0], diff_shape), int_to_idx[\"<PAD>\"])\n        \n        # Concatenate \n        y = torch.cat((y, y_pad), 1)\n        \n        optimizer.zero_grad()\n        \n        \n        # Initialize the state h_0\n        prev_state = model.init_state(batch_size=batch_size)\n        prev_state = prev_state.to(DEVICE)\n            \n        # Forward\n        X = X.to(DEVICE)\n        y_pred, h = model(X, prev_state)\n        \n        \n\n        # Calculate loss\n        y = y.to(DEVICE)\n        loss = criterion(y_pred.transpose(1, 2), y)  \n        running_loss += loss.item()\n\n\n        # Calculate gradients and update parameters\n        loss.backward()\n        \n        # Clip the gradient \n        nn.utils.clip_grad_norm_(model.parameters(), 1)\n        optimizer.step()\n    \n    if (epoch + 1) % 10 == 0:\n        print(\"Epoch {}, Loss = {}\".format(epoch + 1, running_loss / len(dataloader_train)))","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:31:26.688705Z","iopub.execute_input":"2024-10-28T11:31:26.689219Z","iopub.status.idle":"2024-10-28T11:37:08.235354Z","shell.execute_reply.started":"2024-10-28T11:31:26.689172Z","shell.execute_reply":"2024-10-28T11:37:08.234392Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 10, Loss = 1.5284101007461548\nEpoch 20, Loss = 1.520075101661682\nEpoch 30, Loss = 1.5144786883354187\nEpoch 40, Loss = 1.5099656485557555\nEpoch 50, Loss = 1.507754513168335\nEpoch 60, Loss = 1.506077953338623\nEpoch 70, Loss = 1.5053257856369018\nEpoch 80, Loss = 1.5032473198890686\nEpoch 90, Loss = 1.5036709668159485\nEpoch 100, Loss = 1.5020613679885864\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Test set","metadata":{}},{"cell_type":"code","source":"model.to(\"cpu\")\ntest_size = 1000\nX_int_test = generate_data(test_size, n_min = n_min, n_max = n_max)\ncorrect = 0\ndataset_test = IntDataset(X_int_test, int_to_idx)\nfor i, (X, y) in enumerate(dataset_test):\n    optimizer.zero_grad()\n        \n        \n    # Initialize the state h_0\n    prev_state = model.init_state(batch_size=1)\n\n    # Forward\n    y_pred, h = model(X, prev_state)\n    \n    _, predicted = torch.max(y_pred.data, 1)\n    \n    out = \"\"\n    for x in predicted:\n        if x == int_to_idx[\"<EOS>\"]:\n            break\n        out += str(idx_to_int[int(x)])\n    \n    \n    out = int(out)\n    \n    if out == X_int_test[i, 0] + X_int_test[i, 1]:\n        correct += 1\n    \nprint(\"Accuracy:\", 100 * correct / test_size, \"%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:37:08.236627Z","iopub.execute_input":"2024-10-28T11:37:08.236943Z","iopub.status.idle":"2024-10-28T11:37:09.238062Z","shell.execute_reply.started":"2024-10-28T11:37:08.236910Z","shell.execute_reply":"2024-10-28T11:37:09.237090Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy: 0.5 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Custom numbers\n\nIn this section you can sum your favourite integers","metadata":{}},{"cell_type":"code","source":"A = np.random.randint(n_min, n_max)\nB = np.random.randint(n_min, n_max)\nC = A + B \n\nX = []\ny = []\nA = str(A)\nB = str(B)\n\n # Take the maximum length \nlength_max = max(len(A), len(B))\n\n# Pad both to the maximal lengh\nwhile len(A) < length_max:\n    A = '0' + A\nwhile len(B) < length_max:\n    B = '0' + B\n    \nfor c in A:\n    X.append(int_to_idx[c])\n\nX.append(int_to_idx[\"+\"])\n\nfor c in B:\n    X.append(int_to_idx[c])\n\nX.append(int_to_idx[\"<EOS>\"])\n\n # Initialize the state h_0  (no batch)\nprev_state = model.init_state(batch_size=1)\n\n# Forward\ny_pred, h = model(torch.tensor(X), prev_state)\n\n_, predicted = torch.max(y_pred.data, 1)\n\nout = A + \" + \" +  B + \" = \"\nfor x in predicted:\n    if x == int_to_idx[\"<EOS>\"]:\n        break\n    out += str(idx_to_int[int(x)])\n    \nprint(\" Predicted:\", out, \". Acutual: \", C)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:37:09.241088Z","iopub.execute_input":"2024-10-28T11:37:09.241527Z","iopub.status.idle":"2024-10-28T11:37:09.253249Z","shell.execute_reply.started":"2024-10-28T11:37:09.241493Z","shell.execute_reply":"2024-10-28T11:37:09.252215Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":" Predicted: 33 + 07 = 12 . Acutual:  40\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}