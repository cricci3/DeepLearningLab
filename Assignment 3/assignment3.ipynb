{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To8qY-VceXi-",
        "outputId": "cbad1da4-2fd0-4c5d-eb05-37e389090cdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WOb9l2Nfb5NJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "psg1fhfwb5NK"
      },
      "outputs": [],
      "source": [
        "# Set the seed\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "# Probably, this below must be changed if you work with a M1/M2/M3 Mac\n",
        "torch.cuda.manual_seed(seed) # for CUDA\n",
        "torch.backends.cudnn.deterministic = True # for CUDNN\n",
        "torch.backends.benchmark = False # if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rKdx7lvb5NK"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j0yDnTIb5NL"
      },
      "source": [
        "### Question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286,
          "referenced_widgets": [
            "c27df1ff7b0d4563b093ea6fbe5bb70b",
            "4ecf35f74e0243ffa6692c6dda75ed36",
            "ab9e73d09a45462bab3014bc8fd22202",
            "db4730225cff418289c58c3b27c3b3c4",
            "2edc910f0e4f4ecb9cdd233efba72c07",
            "81e5ce2737f6491783808184ad99ec9c",
            "857880c702d34a26a73226748ca01c6c",
            "9813c578a0964253b871c136d12886b5",
            "54783fe863b344ad90aa8993de651415",
            "a5523a1df2e04c3db620f47db10c3d9f",
            "c02f5c78f54b4c2aa813ecc2bcb77dee",
            "e2e5f1a036d54b1690d348a0678f185f",
            "7039e4cd4d17415aa47558ccfc9b5a74",
            "3fae0fac810449d1a7d8812ffef8276e",
            "79021a3f451f407e9bec0b1ce2c88c58",
            "266bd4ed2bf2491c97ab90f364792d77",
            "19ba8f25e5c74266b8360ca80adfcbba",
            "b8be9737f7064421b82ab61aa98dadb1",
            "ccb05a3ccd5a45be9891a1e16da86a8b",
            "9bb8e83c34274f349563c790e898a489",
            "87973eb4560a470489c2ce59fa316922",
            "05d6bc34eca349828bda5fd9dac46400",
            "c840b34bec1e4f66912fccc1b2cbfdfe",
            "10f6e8e26db842c0a4c6756b6e99c783",
            "da0317fdfef144b48fd15a3c11728081",
            "c9038dfe0e0e4ca1a083b3c546819e7e",
            "883df76064aa4ed98b2416de13d116d3",
            "ddc0754057324cecb542d49c05395dac",
            "c506084b879f47c8b507c7d0d7278536",
            "631685d2ef7f4c9b921009e3fdfb0194",
            "3786a0f3eefa423fb36bfc698521f680",
            "f1963c773cea4556b419b0f7e97599e7",
            "52cff869a57f45c7a832d17be3bd053c"
          ]
        },
        "id": "euOVPbH3b5NL",
        "outputId": "467a911f-38ee-495d-e6a3-95eb7b315651"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c27df1ff7b0d4563b093ea6fbe5bb70b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/101 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2e5f1a036d54b1690d348a0678f185f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data.json:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c840b34bec1e4f66912fccc1b2cbfdfe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/209527 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['link', 'headline', 'category', 'short_description', 'authors', 'date'],\n",
            "    num_rows: 209527\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Question 1\n",
        "ds = load_dataset(\"heegyu/news-category-dataset\")\n",
        "print(ds['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRAG91H2b5NM"
      },
      "source": [
        "### Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjtH4IqKb5NM",
        "outputId": "97ac0d7c-72fe-44cd-f68a-3e44c5a63da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First headline (before processing): Biden Says U.S. Forces Would Defend Taiwan If China Invaded\n"
          ]
        }
      ],
      "source": [
        "# Question 2\n",
        "# Filter for \"POLITICS\" category and store each headline as a string in ds_train\n",
        "ds_train = [news['headline'] for news in ds['train'] if news['category'] == 'POLITICS']\n",
        "\n",
        "assert len(ds_train) == 35602\n",
        "\n",
        "print(\"First headline (before processing):\", ds_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbD71IPob5NN"
      },
      "source": [
        "### Question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApoquVdDb5NN",
        "outputId": "3302714f-4489-4620-a18b-a8faadd31716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "biden says u.s. forces would defend taiwan if china invaded\n",
            "['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded']\n"
          ]
        }
      ],
      "source": [
        "# Convert each headline to lowercase\n",
        "ds_train = [headline.lower() for headline in ds_train]\n",
        "\n",
        "# Check the result\n",
        "print(ds_train[0])\n",
        "\n",
        "# Split each headline in words\n",
        "# maybe I could use a better tokenizer (ex. remove all punctation)\n",
        "ds_train = [headline.split(\" \") for headline in ds_train]\n",
        "\n",
        "# Check the result\n",
        "print(ds_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unpTS_74b5NO",
        "outputId": "bf5f606c-2a9e-4a43-fd31-8a1b543a8cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded', '<EOS>']\n"
          ]
        }
      ],
      "source": [
        "# Add <EOS> at the end of every headline\n",
        "for headline in ds_train:\n",
        "    headline.append('<EOS>')\n",
        "\n",
        "# Check the result\n",
        "print(ds_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idd465iLb5NO",
        "outputId": "bbe0f703-e8bc-410d-f5ff-56819eb99596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded', '<EOS>'], ['‘beautiful', 'and', 'sad', 'at', 'the', 'same', 'time’:', 'ukrainian', 'cultural', 'festival', 'takes', 'on', 'a', 'deeper', 'meaning', 'this', 'year', '<EOS>'], ['biden', 'says', \"queen's\", 'death', 'left', \"'giant\", \"hole'\", 'for', 'royal', 'family', '<EOS>'], ['bill', 'to', 'help', 'afghans', 'who', 'escaped', 'taliban', 'faces', 'long', 'odds', 'in', 'the', 'senate', '<EOS>'], ['mark', 'meadows', 'complies', 'with', 'justice', 'dept.', 'subpoena:', 'report', '<EOS>']]\n"
          ]
        }
      ],
      "source": [
        "print(ds_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-8j5hYib5NO"
      },
      "source": [
        "### Question 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNhbLirUb5NO",
        "outputId": "5951c91a-f49a-4169-b7db-ddb7dd4298b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5 most common words: [('<EOS>', 35602), ('to', 10701), ('the', 9618), ('trump', 6895), ('of', 5536)]\n"
          ]
        }
      ],
      "source": [
        "# Flatten ds_train and extract all words (including <EOS> and PAD tokens)\n",
        "all_words = [word for headline in ds_train for word in headline]\n",
        "\n",
        "# Count word frequencies\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Get the 5 most common words\n",
        "most_common_words = word_counts.most_common(5)\n",
        "\n",
        "# Print the 5 most common words\n",
        "print(\"5 most common words:\", most_common_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMxljjTDb5NO",
        "outputId": "5289c1ba-0970-4791-fb3a-6b391264c0b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in vocabulary (excluding <EOS> and PAD): 33230\n",
            "Total number of words in vocabulary after filtering (excluding <EOS> and PAD): 7034\n"
          ]
        }
      ],
      "source": [
        "# Flatten ds_train and extract unique words\n",
        "unique_words = set(word for headline in ds_train for word in headline)\n",
        "\n",
        "# Create vocabulary with <EOS> at the beginning and PAD at the end and remove evenutally alredy presents special tokens\n",
        "unique_words = {word for word in unique_words if word and word not in [\"<EOS>\", \"PAD\"]}\n",
        "\n",
        "# Sorting of unique_words\n",
        "word_vocab = [\"<EOS>\"] + sorted(list(unique_words)) + [\"PAD\"]\n",
        "\n",
        "# Total number of unique words (excluding <EOS> and PAD)\n",
        "total_words = len(word_vocab) - 2\n",
        "\n",
        "# Print the total number of words in the vocabulary\n",
        "print(\"Total number of words in vocabulary (excluding <EOS> and PAD):\", total_words)\n",
        "\n",
        "# Remove words that are used less than a threshold (5 times):\n",
        "threshold = 5\n",
        "filtered_words = {word for word, count in word_counts.items() if count >= threshold}\n",
        "filtered_word_vocab = [\"<EOS>\"] + sorted(list(filtered_words)) + [\"PAD\"]\n",
        "\n",
        "# Number of unique words after filtering (excluding <EOS> and PAD)\n",
        "total_words = len(filtered_word_vocab) - 2\n",
        "\n",
        "# Print the total number of words in the vocabulary\n",
        "print(\"Total number of words in vocabulary after filtering (excluding <EOS> and PAD):\", total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GWy5SAlnb5NP"
      },
      "outputs": [],
      "source": [
        "# Dictionary representing a mapping from words of our word_vocab to integer values\n",
        "word_to_int = {word: i for i, word in enumerate(word_vocab)}\n",
        "\n",
        "assert word_to_int['<EOS>'] == 0 and word_to_int['PAD'] == len(word_vocab) - 1\n",
        "#print(f\"<EOS> index: {word_to_int['<EOS>']}\")\n",
        "#print(f\"PAD index: {word_to_int['PAD']}\")\n",
        "\n",
        "# print(\"Sample mapping:\", list(word_to_int.items())[:10])  # Print first 10 mappings\n",
        "\n",
        "# Dictionary representing the inverse of `word_to_int`, i.e. a mapping from integer (keys) to characters (values).\n",
        "int_to_word = {word:i for i, word in word_to_int.items()}\n",
        "\n",
        "assert int_to_word[0] == '<EOS>' and int_to_word[len(word_vocab)-1] == 'PAD'\n",
        "#print(f\"Word at first index (0): {int_to_word[0]}\")\n",
        "#print(f\"Word at last index ({len(word_vocab)-1}): {int_to_word[len(word_vocab)-1]}\")\n",
        "\n",
        "# print(\"Sample mapping:\", list(int_to_word.items())[:10])  # Print first 10 mappings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETTonTnlb5NP"
      },
      "source": [
        "### Question 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9IQCtHnUb5NP"
      },
      "outputs": [],
      "source": [
        "# Create a dataset class\n",
        "# - input:\n",
        "#       list of tokenized sequences\n",
        "#       word_to_int\n",
        "# - Each item: a tuple having\n",
        "#        the indexes of all the words of the sentence except the last one;\n",
        "#        all the elements of that sentence except the first one\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, sequences, word_to_int):\n",
        "        self.sequences = sequences\n",
        "        self.word_to_int = word_to_int\n",
        "\n",
        "        # Convert each sequence (list of words) to indexes using map\n",
        "        self.indexed_sequences = [\n",
        "            [self.word_to_int[word] for word in sequence if word in self.word_to_int]\n",
        "            for sequence in self.sequences\n",
        "        ] # the problem is that if in the sequence there is a word (ex '') without mapping, skip it\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the indexed sequence at the given index\n",
        "        indexed_seq = self.indexed_sequences[idx]\n",
        "\n",
        "        # Create x (all indexes except the last one) and y (all indexes except the first one)\n",
        "        x = indexed_seq[:-1]\n",
        "        y = indexed_seq[1:]\n",
        "\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of sequences\n",
        "        return len(self.indexed_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK54MnTab5NP"
      },
      "source": [
        "### Question 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IZendFDTb5NP"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, pad_value):\n",
        "  # Separate data (x) and target (y) pairs from the batch\n",
        "  data, targets = zip(*batch)\n",
        "\n",
        "  padded_data = nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=pad_value)\n",
        "  padded_targets = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=pad_value)\n",
        "\n",
        "  return padded_data, padded_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j98SvODmb5NP"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataset = Dataset(ds_train, word_to_int)\n",
        "\n",
        "if batch_size == 1:\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "else:\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
        "                          collate_fn=lambda batch: collate_fn(batch, word_to_int[\"PAD\"]))\n",
        "\n",
        "  # By default, DataLoader expects a function like collate_fn(batch) that takes only one argument—the batch itself.\n",
        "  # However, in this case, collate_fn requires an additional argument (pad_value).\n",
        "  # The lambda function allows to rewrite collate_fn(batch, pad_value) into a version compatible with DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFGFn8FCb5NP"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y-SnaDznb5NP"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, map, hidden_size=1024, emb_dim=150, n_layers=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.vocab_size  = len(map)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.emb_dim     = emb_dim\n",
        "        self.n_layers    = n_layers\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=self.vocab_size,\n",
        "            embedding_dim=self.emb_dim,\n",
        "            padding_idx=map[\"PAD\"]\n",
        "        )\n",
        "\n",
        "        # LSTM layer with potential stacking\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.emb_dim,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.n_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Fully connected layer to project LSTM outputs to vocabulary size\n",
        "        self.fc = nn.Linear(\n",
        "            in_features=self.hidden_size,\n",
        "            out_features=self.vocab_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        # Embedding lookup for input tokens\n",
        "        embed = self.embedding(x)\n",
        "\n",
        "        # Pass embeddings through the LSTM\n",
        "        yhat, state = self.lstm(embed, prev_state)  # yhat: (batch, seq_length, hidden_size)\n",
        "\n",
        "        # Pass through the fully connected layer to get logits\n",
        "        out = self.fc(yhat)  # out: (batch, seq_length, vocab_size)\n",
        "\n",
        "        return out, state\n",
        "\n",
        "    def init_state(self, b_size=1):\n",
        "        # Initializes hidden and cell states with zeros\n",
        "        # Each state has shape (n_layers, batch_size, hidden_size)\n",
        "        return (torch.zeros(self.n_layers, b_size, self.hidden_size),\n",
        "                torch.zeros(self.n_layers, b_size, self.hidden_size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkxgye6Hb5NQ"
      },
      "source": [
        "## Evaluation 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czzpIlYgb5NQ"
      },
      "source": [
        "$random\\_sample\\_next$ which randomly sample the next word on $p(wn|w0, w1, . . . , wn−1)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C3tTRb7jb5NQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def random_sample_next(model, x, prev_state, topk=None):\n",
        "    \"\"\"\n",
        "    Randomly samples the next word based on the probability distribution.\n",
        "\n",
        "    Args:\n",
        "        model: Trained LSTM model.\n",
        "        x: Input tensor of shape (batch_size, seq_length).\n",
        "        prev_state: Previous hidden and cell state of the LSTM.\n",
        "        topk: Number of top candidates to consider for sampling. Defaults to all if None.\n",
        "\n",
        "    Returns:\n",
        "        sampled_ix: Index of the randomly sampled word.\n",
        "        state: Updated LSTM state after processing the input.\n",
        "    \"\"\"\n",
        "    # Perform forward-prop and get the output of the last time-step\n",
        "    out, state = model(x, prev_state)\n",
        "    last_out = out[0, -1, :]  # Vocabulary values of last element of sequence\n",
        "\n",
        "    # if topk is not None:\n",
        "\n",
        "    # Get the top-k indexes and their values\n",
        "    topk = topk if topk else last_out.shape[0]\n",
        "    top_logit, top_ix = torch.topk(last_out, k=topk, dim=-1)\n",
        "    # Convert logits to probabilities and sample\n",
        "    p = F.softmax(top_logit.detach(), dim=-1).numpy()\n",
        "\n",
        "    # Check if top_ix is empty\n",
        "    if len(top_ix) == 0:\n",
        "        raise ValueError(\"No valid predictions were made (top_ix is empty).\")\n",
        "\n",
        "    sampled_ix = np.random.choice(top_ix.numpy(), p=p)\n",
        "\n",
        "    #else:\n",
        "        # Use all logits for sampling\n",
        "        # p = F.softmax(last_out.detach(), dim=-1).numpy()\n",
        "        # sampled_ix = np.random.choice(np.arange(len(p)), p=p)\n",
        "\n",
        "    return sampled_ix, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWuKKum2b5NQ"
      },
      "source": [
        "$sample\\_argmax$ which picks the word having the highest probability according to the distribution $p(wn|w0, w1, . . . , wn−1)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8OsamFaQb5NQ"
      },
      "outputs": [],
      "source": [
        "def sample_argmax(model, x, prev_state):\n",
        "    \"\"\"\n",
        "    Samples the next word by picking the one with the highest probability (argmax strategy).\n",
        "\n",
        "    Args:\n",
        "        model: Trained LSTM model.\n",
        "        x: Input tensor of shape (batch_size, seq_length).\n",
        "        prev_state: Previous hidden and cell state of the LSTM.\n",
        "\n",
        "    Returns:\n",
        "        sampled_ix: Index of the word with the highest probability.\n",
        "        state: Updated LSTM state after processing the input.\n",
        "    \"\"\"\n",
        "    # Perform forward-prop and get the output of the last time-step\n",
        "    out, state = model(x, prev_state)\n",
        "    last_out = out[0, -1, :]  # Vocabulary values of last element of sequence\n",
        "\n",
        "    # Get the index with the highest probability\n",
        "    sampled_ix = torch.argmax(last_out).item()\n",
        "\n",
        "    return sampled_ix, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1DSHZJGb5NQ"
      },
      "source": [
        "$sample$ takes as minimal input: a prompt (some words), the model, and one of the two functions above defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pwbrxSjNb5NQ"
      },
      "outputs": [],
      "source": [
        "def sample(model, seed, strategy=\"random\", topk=5, max_seqlen=18, stop_on=None):\n",
        "    \"\"\"\n",
        "    Generates a sequence using the model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained LSTM model.\n",
        "        seed: Initial list of token indices to start generation.\n",
        "        strategy: Sampling strategy - 'random' or 'max'.\n",
        "        topk: Number of top candidates to consider for 'random' sampling.\n",
        "        max_seqlen: Maximum sequence length to generate.\n",
        "        stop_on: Token index to stop generation.\n",
        "\n",
        "    Returns:\n",
        "        sampled_ix_list: List of token indices for the generated sequence.\n",
        "    \"\"\"\n",
        "    # the model expect that seed (prompt) is a list or a tuple to iter on it. But if it is a single int transform it in the correct form\n",
        "    seed = seed if isinstance(seed, (list, tuple)) else [seed]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sampled_ix_list = seed[:]\n",
        "        x = torch.tensor([seed])\n",
        "\n",
        "        prev_state = model.init_state(b_size=1)\n",
        "        for _ in range(max_seqlen - len(seed)):\n",
        "            # Repeatedly predicts the next word/token based on the input sequence\n",
        "            if strategy == \"random\":\n",
        "                sampled_ix, prev_state = random_sample_next(model, x, prev_state, topk)\n",
        "            else:\n",
        "                sampled_ix, prev_state = sample_argmax(model, x, prev_state)\n",
        "\n",
        "            # The predicted token is appended to the sequence\n",
        "            sampled_ix_list.append(sampled_ix)\n",
        "\n",
        "            # The new token is used as the input for the next prediction\n",
        "            x = torch.tensor([[sampled_ix]])\n",
        "\n",
        "            # If the predicted token is word_to_int[\"<EOS>\"] the function terminates the loop\n",
        "            if stop_on is not None and sampled_ix == stop_on:\n",
        "                break\n",
        "\n",
        "    model.train()\n",
        "    return sampled_ix_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vrVcYjbb5NQ"
      },
      "source": [
        "Prior to training, evaluate your model generating and reporting here 2/3 sentences in the following way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_IP4vRotb5NQ"
      },
      "outputs": [],
      "source": [
        "model = LSTMModel(word_to_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGHp7mi6cCBU",
        "outputId": "67b48653-b60c-45c2-fb50-5130edf5e46f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps'\n",
        "        if torch.backends.mps.is_available() else 'cpu')\n",
        "model = model.to(DEVICE)\n",
        "print(\"Working on\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "e5Xyt96Pb5NQ"
      },
      "outputs": [],
      "source": [
        "# Function to tokenize and map words to indices\n",
        "def tokenize_and_map(sentence, word_to_int):\n",
        "    tokens = sentence.split(\" \")  # Split sentence into words\n",
        "    return [word_to_int[word] for word in tokens if word in word_to_int]\n",
        "\n",
        "def decode_sequence(generated_tokens, int_to_word):\n",
        "    # Convert a list of token IDs into words using the int_to_word mapping\n",
        "    return [int_to_word[token] for token in generated_tokens if token in int_to_word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "RLs3gOWab5NR",
        "outputId": "e1c1e165-35b0-4f80-ab59-ce7bb0516814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed: [29513, 23553, 31678]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-0ffc75aac747>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<EOS>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-4ad1acfe8f14>\u001b[0m in \u001b[0;36msample\u001b[0;34m(model, seed, strategy, topk, max_seqlen, stop_on)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Repeatedly predicts the next word/token based on the input sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"random\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0msampled_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_sample_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0msampled_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_argmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a78503f5dc12>\u001b[0m in \u001b[0;36mrandom_sample_next\u001b[0;34m(model, x, prev_state, topk)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Perform forward-prop and get the output of the last time-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlast_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Vocabulary values of last element of sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-248e70b8fb7d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prev_state)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Embedding lookup for input tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Pass embeddings through the LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ],
      "source": [
        "# Start with any prompt, e.g., “the president wants”, and generate three sentences with the sampling strategy.\n",
        "seed = \"the president wants\"\n",
        "seed = tokenize_and_map(seed, word_to_int)\n",
        "print(f\"Seed: {seed}\")\n",
        "\n",
        "for i in range(3):\n",
        "    generated = sample(model, seed, \"random\", word_to_int[\"<EOS>\"])\n",
        "    generated = decode_sequence(generated, int_to_word)\n",
        "    print(\"Generated: \", generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYLPZjBYb5NR"
      },
      "outputs": [],
      "source": [
        "# Start with any prompt, e.g., “the president wants”, and generate three sentences with the sampling strategy.\n",
        "seed = \"the president wants\"\n",
        "seed = tokenize_and_map(seed, word_to_int)  # Convert to token indices\n",
        "print(f\"Seed: {seed}\")\n",
        "\n",
        "for i in range(3):\n",
        "    generated = sample(model, seed, \"random\", word_to_int[\"<EOS>\"])\n",
        "    generated = decode_sequence(generated, int_to_word)\n",
        "    print(\"Generated: \", generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4apAckjb5NR"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3aJQBwMb5NR",
        "outputId": "0e7d6bd1-5fe7-4f5a-def9-b25814676661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2/12, Loss:   6.3853\n",
            "Epoch: 4/12, Loss:   6.1057\n",
            "Epoch: 6/12, Loss:   5.8916\n",
            "Epoch: 8/12, Loss:   5.7213\n",
            "Epoch: 10/12, Loss:   5.5822\n",
            "Epoch: 12/12, Loss:   5.4669\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "\n",
        "costs = []\n",
        "running_loss = 0\n",
        "loss_hist = []\n",
        "\n",
        "lr=1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word_to_int[\"PAD\"])\n",
        "clip=1\n",
        "\n",
        "print_every=2\n",
        "num_epochs = 12\n",
        "epoch = 0\n",
        "\n",
        "while epoch<num_epochs:\n",
        "    epoch += 1\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.long().to(DEVICE), y.long().to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        # Initialise model's state and perform forward-prop\n",
        "        prev_state = model.init_state(b_size=x.shape[0])\n",
        "        prev_state = tuple(s.to(DEVICE) for s in prev_state)  # to DEVICE\n",
        "        out, state = model(x, prev_state)         # out has dim: batch x seq_length x vocab_size\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(out.transpose(1, 2), y)  #transpose is required to obtain batch x vocab_size x seq_length            costs.append(loss.item())\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate gradients and update parameters\n",
        "        loss.backward()\n",
        "        if clip:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    if print_every and (epoch%print_every)==0:\n",
        "        print(\"Epoch: {}/{}, Loss: {:8.4f}\".format(\n",
        "                int(epoch), int(num_epochs),\n",
        "                running_loss/float(print_every*len(dataloader))))\n",
        "        loss_hist.append(running_loss/float(print_every*len(dataloader)))\n",
        "        running_loss = 0\n",
        "\n",
        "    if loss < 1.5:\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deeple",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05d6bc34eca349828bda5fd9dac46400": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10f6e8e26db842c0a4c6756b6e99c783": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddc0754057324cecb542d49c05395dac",
            "placeholder": "​",
            "style": "IPY_MODEL_c506084b879f47c8b507c7d0d7278536",
            "value": "Generating train split: 100%"
          }
        },
        "19ba8f25e5c74266b8360ca80adfcbba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "266bd4ed2bf2491c97ab90f364792d77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2edc910f0e4f4ecb9cdd233efba72c07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3786a0f3eefa423fb36bfc698521f680": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fae0fac810449d1a7d8812ffef8276e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccb05a3ccd5a45be9891a1e16da86a8b",
            "max": 87295572,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bb8e83c34274f349563c790e898a489",
            "value": 87295572
          }
        },
        "4ecf35f74e0243ffa6692c6dda75ed36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81e5ce2737f6491783808184ad99ec9c",
            "placeholder": "​",
            "style": "IPY_MODEL_857880c702d34a26a73226748ca01c6c",
            "value": "README.md: 100%"
          }
        },
        "52cff869a57f45c7a832d17be3bd053c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54783fe863b344ad90aa8993de651415": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "631685d2ef7f4c9b921009e3fdfb0194": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7039e4cd4d17415aa47558ccfc9b5a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19ba8f25e5c74266b8360ca80adfcbba",
            "placeholder": "​",
            "style": "IPY_MODEL_b8be9737f7064421b82ab61aa98dadb1",
            "value": "data.json: 100%"
          }
        },
        "79021a3f451f407e9bec0b1ce2c88c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87973eb4560a470489c2ce59fa316922",
            "placeholder": "​",
            "style": "IPY_MODEL_05d6bc34eca349828bda5fd9dac46400",
            "value": " 87.3M/87.3M [00:01&lt;00:00, 54.3MB/s]"
          }
        },
        "81e5ce2737f6491783808184ad99ec9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "857880c702d34a26a73226748ca01c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87973eb4560a470489c2ce59fa316922": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "883df76064aa4ed98b2416de13d116d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9813c578a0964253b871c136d12886b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bb8e83c34274f349563c790e898a489": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5523a1df2e04c3db620f47db10c3d9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9e73d09a45462bab3014bc8fd22202": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9813c578a0964253b871c136d12886b5",
            "max": 101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54783fe863b344ad90aa8993de651415",
            "value": 101
          }
        },
        "b8be9737f7064421b82ab61aa98dadb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c02f5c78f54b4c2aa813ecc2bcb77dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c27df1ff7b0d4563b093ea6fbe5bb70b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ecf35f74e0243ffa6692c6dda75ed36",
              "IPY_MODEL_ab9e73d09a45462bab3014bc8fd22202",
              "IPY_MODEL_db4730225cff418289c58c3b27c3b3c4"
            ],
            "layout": "IPY_MODEL_2edc910f0e4f4ecb9cdd233efba72c07"
          }
        },
        "c506084b879f47c8b507c7d0d7278536": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c840b34bec1e4f66912fccc1b2cbfdfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10f6e8e26db842c0a4c6756b6e99c783",
              "IPY_MODEL_da0317fdfef144b48fd15a3c11728081",
              "IPY_MODEL_c9038dfe0e0e4ca1a083b3c546819e7e"
            ],
            "layout": "IPY_MODEL_883df76064aa4ed98b2416de13d116d3"
          }
        },
        "c9038dfe0e0e4ca1a083b3c546819e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1963c773cea4556b419b0f7e97599e7",
            "placeholder": "​",
            "style": "IPY_MODEL_52cff869a57f45c7a832d17be3bd053c",
            "value": " 209527/209527 [00:02&lt;00:00, 96806.47 examples/s]"
          }
        },
        "ccb05a3ccd5a45be9891a1e16da86a8b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da0317fdfef144b48fd15a3c11728081": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_631685d2ef7f4c9b921009e3fdfb0194",
            "max": 209527,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3786a0f3eefa423fb36bfc698521f680",
            "value": 209527
          }
        },
        "db4730225cff418289c58c3b27c3b3c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5523a1df2e04c3db620f47db10c3d9f",
            "placeholder": "​",
            "style": "IPY_MODEL_c02f5c78f54b4c2aa813ecc2bcb77dee",
            "value": " 101/101 [00:00&lt;00:00, 2.57kB/s]"
          }
        },
        "ddc0754057324cecb542d49c05395dac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2e5f1a036d54b1690d348a0678f185f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7039e4cd4d17415aa47558ccfc9b5a74",
              "IPY_MODEL_3fae0fac810449d1a7d8812ffef8276e",
              "IPY_MODEL_79021a3f451f407e9bec0b1ce2c88c58"
            ],
            "layout": "IPY_MODEL_266bd4ed2bf2491c97ab90f364792d77"
          }
        },
        "f1963c773cea4556b419b0f7e97599e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
