{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "# Probably, this below must be changed if you work with a M1/M2/M3 Mac\n",
    "torch.cuda.manual_seed(seed) # for CUDA\n",
    "torch.backends.cudnn.deterministic = True # for CUDNN\n",
    "torch.backends.benchmark = False # if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['link', 'headline', 'category', 'short_description', 'authors', 'date'],\n",
      "    num_rows: 209527\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "ds = load_dataset(\"heegyu/news-category-dataset\")\n",
    "print(ds['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First headline (before processing): Biden Says U.S. Forces Would Defend Taiwan If China Invaded\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "# Filter for \"POLITICS\" category and store each headline as a string in ds_train\n",
    "ds_train = [news['headline'] for news in ds['train'] if news['category'] == 'POLITICS']\n",
    "\n",
    "assert len(ds_train) == 35602\n",
    "\n",
    "print(\"First headline (before processing):\", ds_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biden says u.s. forces would defend taiwan if china invaded\n",
      "['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded']\n"
     ]
    }
   ],
   "source": [
    "# Convert each headline to lowercase\n",
    "ds_train = [headline.lower() for headline in ds_train]\n",
    "\n",
    "# Check the result\n",
    "print(ds_train[0])\n",
    "\n",
    "# Split each headline in words\n",
    "# maybe I could use a better tokenizer (ex. remove all punctation)\n",
    "ds_train = [headline.split(\" \") for headline in ds_train]\n",
    "\n",
    "# Check the result\n",
    "print(ds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Add <EOS> at the end of every headline\n",
    "for headline in ds_train:\n",
    "    headline.append('<EOS>')\n",
    "\n",
    "# Check the result\n",
    "print(ds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded', '<EOS>'], ['‘beautiful', 'and', 'sad', 'at', 'the', 'same', 'time’:', 'ukrainian', 'cultural', 'festival', 'takes', 'on', 'a', 'deeper', 'meaning', 'this', 'year', '<EOS>'], ['biden', 'says', \"queen's\", 'death', 'left', \"'giant\", \"hole'\", 'for', 'royal', 'family', '<EOS>'], ['bill', 'to', 'help', 'afghans', 'who', 'escaped', 'taliban', 'faces', 'long', 'odds', 'in', 'the', 'senate', '<EOS>'], ['mark', 'meadows', 'complies', 'with', 'justice', 'dept.', 'subpoena:', 'report', '<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print(ds_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words: [('<EOS>', 35602), ('to', 10701), ('the', 9618), ('trump', 6895), ('of', 5536)]\n"
     ]
    }
   ],
   "source": [
    "# Flatten ds_train and extract all words (including <EOS> and PAD tokens)\n",
    "all_words = [word for headline in ds_train for word in headline]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Get the 5 most common words\n",
    "most_common_words = word_counts.most_common(5)\n",
    "\n",
    "# Print the 5 most common words\n",
    "print(\"5 most common words:\", most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in vocabulary (excluding <EOS> and PAD): 33230\n",
      "Total number of words in vocabulary after filtering (excluding <EOS> and PAD): 7034\n"
     ]
    }
   ],
   "source": [
    "# Flatten ds_train and extract unique words \n",
    "unique_words = set(word for headline in ds_train for word in headline)\n",
    "\n",
    "# Create vocabulary with <EOS> at the beginning and PAD at the end and remove evenutally alredy presents special tokens\n",
    "unique_words = {word for word in unique_words if word and word not in [\"<EOS>\", \"PAD\"]}\n",
    "\n",
    "# Sorting of unique_words\n",
    "word_vocab = [\"<EOS>\"] + sorted(list(unique_words)) + [\"PAD\"]\n",
    "\n",
    "# Total number of unique words (excluding <EOS> and PAD)\n",
    "total_words = len(word_vocab) - 2\n",
    "\n",
    "# Print the total number of words in the vocabulary\n",
    "print(\"Total number of words in vocabulary (excluding <EOS> and PAD):\", total_words)\n",
    "\n",
    "# Remove words that are used less than a threshold (5 times):\n",
    "threshold = 5\n",
    "filtered_words = {word for word, count in word_counts.items() if count >= threshold}\n",
    "filtered_word_vocab = [\"<EOS>\"] + sorted(list(filtered_words)) + [\"PAD\"]\n",
    "\n",
    "# Number of unique words after filtering (excluding <EOS> and PAD)\n",
    "total_words = len(filtered_word_vocab) - 2\n",
    "\n",
    "# Print the total number of words in the vocabulary\n",
    "print(\"Total number of words in vocabulary after filtering (excluding <EOS> and PAD):\", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary representing a mapping from words of our word_vocab to integer values\n",
    "word_to_int = {word: i for i, word in enumerate(word_vocab)}\n",
    "\n",
    "assert word_to_int['<EOS>'] == 0 and word_to_int['PAD'] == len(word_vocab) - 1\n",
    "#print(f\"<EOS> index: {word_to_int['<EOS>']}\")\n",
    "#print(f\"PAD index: {word_to_int['PAD']}\")\n",
    "\n",
    "# print(\"Sample mapping:\", list(word_to_int.items())[:10])  # Print first 10 mappings\n",
    "\n",
    "# Dictionary representing the inverse of `word_to_int`, i.e. a mapping from integer (keys) to characters (values).\n",
    "int_to_word = {word:i for i, word in word_to_int.items()}\n",
    "\n",
    "assert int_to_word[0] == '<EOS>' and int_to_word[len(word_vocab)-1] == 'PAD'\n",
    "#print(f\"Word at first index (0): {int_to_word[0]}\")\n",
    "#print(f\"Word at last index ({len(word_vocab)-1}): {int_to_word[len(word_vocab)-1]}\")\n",
    "\n",
    "# print(\"Sample mapping:\", list(int_to_word.items())[:10])  # Print first 10 mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset class\n",
    "# - input:\n",
    "#       list of tokenized sequences\n",
    "#       word_to_int\n",
    "# - Each item: a tuple having\n",
    "#        the indexes of all the words of the sentence except the last one;\n",
    "#        all the elements of that sentence except the first one\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, sequences, word_to_int):\n",
    "        self.sequences = sequences\n",
    "        self.word_to_int = word_to_int\n",
    "\n",
    "        # Convert each sequence (list of words) to indexes using map\n",
    "        self.indexed_sequences = [\n",
    "            [self.word_to_int[word] for word in sequence if word in self.word_to_int] \n",
    "            for sequence in self.sequences\n",
    "        ] # the problem is that if in the sequence there is a word (ex '') without mapping, skip it\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the indexed sequence at the given index\n",
    "        indexed_seq = self.indexed_sequences[idx]\n",
    "        \n",
    "        # Create x (all indexes except the last one) and y (all indexes except the first one)\n",
    "        x = indexed_seq[:-1]\n",
    "        y = indexed_seq[1:]\n",
    "        \n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the total number of sequences\n",
    "        return len(self.indexed_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_value):\n",
    "  # Separate data (x) and target (y) pairs from the batch\n",
    "  data, targets = zip(*batch)\n",
    "\n",
    "  padded_data = nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=pad_value)\n",
    "  padded_targets = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "  return padded_data, padded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = Dataset(ds_train, word_to_int)\n",
    "\n",
    "if batch_size == 1:\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, word_to_int[\"PAD\"]))\n",
    "  \n",
    "  # By default, DataLoader expects a function like collate_fn(batch) that takes only one argument—the batch itself.\n",
    "  # However, in this case, collate_fn requires an additional argument (pad_value).\n",
    "  # The lambda function allows to rewrite collate_fn(batch, pad_value) into a version compatible with DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, map, hidden_size, emb_dim=8, n_layers=1, dropout_p=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.vocab_size  = len(map)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dim     = emb_dim\n",
    "        self.n_layers    = n_layers\n",
    "        self.dropout_p   = dropout_p\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.emb_dim,\n",
    "            padding_idx=map[\"PAD\"]\n",
    "        )\n",
    "\n",
    "        # LSTM layer with potential stacking\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.emb_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_p if n_layers > 1 else 0  # Apply dropout only if more than 1 layer\n",
    "        )\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        # Fully connected layer to project LSTM outputs to vocabulary size\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        # Embedding lookup for input tokens\n",
    "        embed = self.embedding(x)\n",
    "\n",
    "        # Pass embeddings through the LSTM\n",
    "        yhat, state = self.lstm(embed, prev_state)  # yhat: (batch, seq_length, hidden_size)\n",
    "\n",
    "        # Apply dropout to LSTM output\n",
    "        yhat = self.dropout(yhat)\n",
    "\n",
    "        # Pass through the fully connected layer to get logits\n",
    "        out = self.fc(yhat)  # out: (batch, seq_length, vocab_size)\n",
    "        \n",
    "        return out, state\n",
    "\n",
    "    def init_state(self, b_size=1):\n",
    "        # Initializes hidden and cell states with zeros\n",
    "        # Each state has shape (n_layers, batch_size, hidden_size)\n",
    "        return (torch.zeros(self.n_layers, b_size, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, b_size, self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$random\\_sample\\_next$ which randomly sample the next word on $p(wn|w0, w1, . . . , wn−1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_sample_next(model, x, prev_state, topk=None):\n",
    "    \"\"\"\n",
    "    Randomly samples the next word based on the probability distribution.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model.\n",
    "        x: Input tensor of shape (batch_size, seq_length).\n",
    "        prev_state: Previous hidden and cell state of the LSTM.\n",
    "        topk: Number of top candidates to consider for sampling. Defaults to all if None.\n",
    "        \n",
    "    Returns:\n",
    "        sampled_ix: Index of the randomly sampled word.\n",
    "        state: Updated LSTM state after processing the input.\n",
    "    \"\"\"\n",
    "    # Perform forward-prop and get the output of the last time-step\n",
    "    out, state = model(x, prev_state)\n",
    "    last_out = out[0, -1, :]  # Vocabulary values of last element of sequence\n",
    "    \n",
    "    # if topk is not None:\n",
    "    \n",
    "    # Get the top-k indexes and their values\n",
    "    topk = topk if topk else last_out.shape[0]\n",
    "    top_logit, top_ix = torch.topk(last_out, k=topk, dim=-1)\n",
    "    # Convert logits to probabilities and sample\n",
    "    p = F.softmax(top_logit.detach(), dim=-1).numpy()\n",
    "\n",
    "    # Check if top_ix is empty\n",
    "    if len(top_ix) == 0:\n",
    "        raise ValueError(\"No valid predictions were made (top_ix is empty).\")\n",
    "        \n",
    "    sampled_ix = np.random.choice(top_ix.numpy(), p=p)\n",
    "    \n",
    "    #else:\n",
    "        # Use all logits for sampling\n",
    "        # p = F.softmax(last_out.detach(), dim=-1).numpy()\n",
    "        # sampled_ix = np.random.choice(np.arange(len(p)), p=p)\n",
    "    \n",
    "    return sampled_ix, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$sample\\_argmax$ which picks the word having the highest probability according to the distribution $p(wn|w0, w1, . . . , wn−1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_argmax(model, x, prev_state):\n",
    "    \"\"\"\n",
    "    Samples the next word by picking the one with the highest probability (argmax strategy).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model.\n",
    "        x: Input tensor of shape (batch_size, seq_length).\n",
    "        prev_state: Previous hidden and cell state of the LSTM.\n",
    "        \n",
    "    Returns:\n",
    "        sampled_ix: Index of the word with the highest probability.\n",
    "        state: Updated LSTM state after processing the input.\n",
    "    \"\"\"\n",
    "    # Perform forward-prop and get the output of the last time-step\n",
    "    out, state = model(x, prev_state)\n",
    "    last_out = out[0, -1, :]  # Vocabulary values of last element of sequence\n",
    "    \n",
    "    # Get the index with the highest probability\n",
    "    sampled_ix = torch.argmax(last_out).item()\n",
    "    \n",
    "    return sampled_ix, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$sample$ takes as minimal input: a prompt (some words), the model, and one of the two functions above defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, seed, strategy=\"random\", topk=5, max_seqlen=18, stop_on=None):\n",
    "    \"\"\"\n",
    "    Generates a sequence using the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model.\n",
    "        seed: Initial list of token indices to start generation.\n",
    "        strategy: Sampling strategy - 'random' or 'max'.\n",
    "        topk: Number of top candidates to consider for 'random' sampling.\n",
    "        max_seqlen: Maximum sequence length to generate.\n",
    "        stop_on: Token index to stop generation.\n",
    "        \n",
    "    Returns:\n",
    "        sampled_ix_list: List of token indices for the generated sequence.\n",
    "    \"\"\"\n",
    "    # the model expect that seed (prompt) is a list or a tuple to iter on it. But if it is a single int transform it in the correct form\n",
    "    seed = seed if isinstance(seed, (list, tuple)) else [seed]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sampled_ix_list = seed[:]\n",
    "        x = torch.tensor([seed])\n",
    "\n",
    "        prev_state = model.init_state(b_size=1)\n",
    "        for _ in range(max_seqlen - len(seed)):\n",
    "            # Repeatedly predicts the next word/token based on the input sequence\n",
    "            if strategy == \"random\":\n",
    "                sampled_ix, prev_state = random_sample_next(model, x, prev_state, topk)\n",
    "            else:\n",
    "                sampled_ix, prev_state = sample_argmax(model, x, prev_state)\n",
    "\n",
    "            # The predicted token is appended to the sequence\n",
    "            sampled_ix_list.append(sampled_ix)\n",
    "\n",
    "            # The new token is used as the input for the next prediction\n",
    "            x = torch.tensor([[sampled_ix]])\n",
    "\n",
    "            # If the predicted token is word_to_int[\"<EOS>\"] the function terminates the loop\n",
    "            if stop_on is not None and sampled_ix == stop_on:\n",
    "                break\n",
    "\n",
    "    model.train()\n",
    "    return sampled_ix_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to training, evaluate your model generating and reporting here 2/3 sentences in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "model = LSTMModel(word_to_int, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and map words to indices\n",
    "def tokenize_and_map(sentence, word_to_int):\n",
    "    tokens = sentence.split(\" \")  # Split sentence into words\n",
    "    return [word_to_int[word] for word in tokens if word in word_to_int]\n",
    "\n",
    "def decode_sequence(generated_tokens, int_to_word):\n",
    "    # Convert a list of token IDs into words using the int_to_word mapping\n",
    "    return [int_to_word[token] for token in generated_tokens if token in int_to_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with any prompt, e.g., “the president wants”, and generate three sentences with the sampling strategy.\n",
    "seed = \"the president wants\"\n",
    "seed = tokenize_and_map(seed, word_to_int)  # Convert to token indices\n",
    "print(f\"Seed: {seed}\")\n",
    "\n",
    "for i in range(3):\n",
    "    generated = sample(model, seed, \"random\", word_to_int[\"<EOS>\"])\n",
    "    generated = decode_sequence(generated, int_to_word)\n",
    "    print(\"Generated: \", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with any prompt, e.g., “the president wants”, and generate three sentences with the sampling strategy.\n",
    "seed = \"the president wants\"\n",
    "seed = tokenize_and_map(seed, word_to_int)  # Convert to token indices\n",
    "print(f\"Seed: {seed}\")\n",
    "\n",
    "for i in range(3):\n",
    "    generated = sample(model, seed, \"random\", word_to_int[\"<EOS>\"])\n",
    "    generated = decode_sequence(generated, int_to_word)\n",
    "    print(\"Generated: \", generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
