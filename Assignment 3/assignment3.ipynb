{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:38:48.316605Z","iopub.status.busy":"2024-11-25T08:38:48.316017Z","iopub.status.idle":"2024-11-25T08:38:57.809329Z","shell.execute_reply":"2024-11-25T08:38:57.808447Z","shell.execute_reply.started":"2024-11-25T08:38:48.316566Z"},"id":"To8qY-VceXi-","outputId":"24a5e455-86ca-48d5-d204-94027704ff49","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:38:57.811289Z","iopub.status.busy":"2024-11-25T08:38:57.811005Z","iopub.status.idle":"2024-11-25T08:39:01.814092Z","shell.execute_reply":"2024-11-25T08:39:01.813354Z","shell.execute_reply.started":"2024-11-25T08:38:57.811261Z"},"id":"WOb9l2Nfb5NJ","trusted":true},"outputs":[],"source":["import torch\n","from datasets import load_dataset\n","from collections import Counter\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:01.815494Z","iopub.status.busy":"2024-11-25T08:39:01.815097Z","iopub.status.idle":"2024-11-25T08:39:01.823344Z","shell.execute_reply":"2024-11-25T08:39:01.822616Z","shell.execute_reply.started":"2024-11-25T08:39:01.815467Z"},"id":"psg1fhfwb5NK","trusted":true},"outputs":[],"source":["# Set the seed\n","seed = 42\n","torch.manual_seed(seed)\n","# Probably, this below must be changed if you work with a M1/M2/M3 Mac\n","torch.cuda.manual_seed(seed) # for CUDA\n","torch.backends.cudnn.deterministic = True # for CUDNN\n","torch.backends.benchmark = False # if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest."]},{"cell_type":"markdown","metadata":{"id":"2rKdx7lvb5NK"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"5j0yDnTIb5NL"},"source":["### Question 1"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:01.825358Z","iopub.status.busy":"2024-11-25T08:39:01.825100Z","iopub.status.idle":"2024-11-25T08:39:04.633501Z","shell.execute_reply":"2024-11-25T08:39:04.632648Z","shell.execute_reply.started":"2024-11-25T08:39:01.825329Z"},"id":"euOVPbH3b5NL","outputId":"6291b50d-8e83-4876-fe28-f865795254ce","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6cade33707fc46fd8f0460e5049bad61","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/101 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76b2423bcf054259acc86b2494d35d7b","version_major":2,"version_minor":0},"text/plain":["data.json:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac0e8561150f42b4b2cdd49b7e5444e9","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/209527 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['link', 'headline', 'category', 'short_description', 'authors', 'date'],\n","    num_rows: 209527\n","})\n"]}],"source":["# Question 1\n","ds = load_dataset(\"heegyu/news-category-dataset\")\n","print(ds['train'])"]},{"cell_type":"markdown","metadata":{"id":"RRAG91H2b5NM"},"source":["### Question 2"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:04.634946Z","iopub.status.busy":"2024-11-25T08:39:04.634523Z","iopub.status.idle":"2024-11-25T08:39:14.200326Z","shell.execute_reply":"2024-11-25T08:39:14.199434Z","shell.execute_reply.started":"2024-11-25T08:39:04.634918Z"},"id":"QjtH4IqKb5NM","outputId":"a170d32a-4a13-46c8-a393-b6ba6982a0b3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["First headline (before processing): Biden Says U.S. Forces Would Defend Taiwan If China Invaded\n"]}],"source":["# Question 2\n","# Filter for \"POLITICS\" category and store each headline as a string in ds_train\n","ds_train = [news['headline'] for news in ds['train'] if news['category'] == 'POLITICS']\n","\n","assert len(ds_train) == 35602\n","\n","print(\"First headline (before processing):\", ds_train[0])"]},{"cell_type":"markdown","metadata":{"id":"MbD71IPob5NN"},"source":["### Question 3"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:14.201562Z","iopub.status.busy":"2024-11-25T08:39:14.201296Z","iopub.status.idle":"2024-11-25T08:39:14.376493Z","shell.execute_reply":"2024-11-25T08:39:14.375606Z","shell.execute_reply.started":"2024-11-25T08:39:14.201536Z"},"id":"ApoquVdDb5NN","outputId":"cd901b4c-27da-4253-8d0e-db7c57cfce6f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["biden says u.s. forces would defend taiwan if china invaded\n","['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded']\n"]}],"source":["# Convert each headline to lowercase\n","ds_train = [headline.lower() for headline in ds_train]\n","\n","# Check the result\n","print(ds_train[0])\n","\n","# Split each headline in words\n","# maybe I could use a better tokenizer (ex. remove all punctation)\n","ds_train = [headline.split(\" \") for headline in ds_train]\n","\n","# Check the result\n","print(ds_train[0])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:14.377711Z","iopub.status.busy":"2024-11-25T08:39:14.377440Z","iopub.status.idle":"2024-11-25T08:39:14.386574Z","shell.execute_reply":"2024-11-25T08:39:14.385517Z","shell.execute_reply.started":"2024-11-25T08:39:14.377673Z"},"id":"unpTS_74b5NO","outputId":"1306420c-f5df-4530-e3ea-057f343415f4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded', '<EOS>']\n"]}],"source":["# Add <EOS> at the end of every headline\n","for headline in ds_train:\n","    headline.append('<EOS>')\n","\n","# Check the result\n","print(ds_train[0])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:14.388032Z","iopub.status.busy":"2024-11-25T08:39:14.387713Z","iopub.status.idle":"2024-11-25T08:39:14.397844Z","shell.execute_reply":"2024-11-25T08:39:14.397043Z","shell.execute_reply.started":"2024-11-25T08:39:14.387999Z"},"id":"Idd465iLb5NO","outputId":"91b279a2-844a-4414-8d90-049d108c8b66","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded', '<EOS>'], ['‘beautiful', 'and', 'sad', 'at', 'the', 'same', 'time’:', 'ukrainian', 'cultural', 'festival', 'takes', 'on', 'a', 'deeper', 'meaning', 'this', 'year', '<EOS>'], ['biden', 'says', \"queen's\", 'death', 'left', \"'giant\", \"hole'\", 'for', 'royal', 'family', '<EOS>'], ['bill', 'to', 'help', 'afghans', 'who', 'escaped', 'taliban', 'faces', 'long', 'odds', 'in', 'the', 'senate', '<EOS>'], ['mark', 'meadows', 'complies', 'with', 'justice', 'dept.', 'subpoena:', 'report', '<EOS>']]\n"]}],"source":["print(ds_train[:5])"]},{"cell_type":"markdown","metadata":{"id":"z-8j5hYib5NO"},"source":["### Question 4"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:14.399025Z","iopub.status.busy":"2024-11-25T08:39:14.398788Z","iopub.status.idle":"2024-11-25T08:39:14.462781Z","shell.execute_reply":"2024-11-25T08:39:14.461922Z","shell.execute_reply.started":"2024-11-25T08:39:14.399002Z"},"id":"NNhbLirUb5NO","outputId":"f4e60181-2fbf-4456-829c-52f822e23547","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5 most common words: [('<EOS>', 35602), ('to', 10701), ('the', 9618), ('trump', 6895), ('of', 5536)]\n"]}],"source":["# Flatten ds_train and extract all words (including <EOS> and PAD tokens)\n","all_words = [word for headline in ds_train for word in headline]\n","\n","# Count word frequencies\n","word_counts = Counter(all_words)\n","\n","# Get the 5 most common words\n","most_common_words = word_counts.most_common(5)\n","\n","# Print the 5 most common words\n","print(\"5 most common words:\", most_common_words)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:14.466030Z","iopub.status.busy":"2024-11-25T08:39:14.465551Z","iopub.status.idle":"2024-11-25T08:39:14.537841Z","shell.execute_reply":"2024-11-25T08:39:14.536857Z","shell.execute_reply.started":"2024-11-25T08:39:14.466001Z"},"id":"YMxljjTDb5NO","outputId":"8764641d-b40a-4dc2-d443-15f781289d5d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of words in vocabulary (excluding <EOS> and PAD): 33230\n","Total number of words in vocabulary after filtering (excluding <EOS> and PAD): 7034\n"]}],"source":["# Flatten ds_train and extract unique words\n","unique_words = set(word for headline in ds_train for word in headline)\n","\n","# Create vocabulary with <EOS> at the beginning and PAD at the end and remove evenutally alredy presents special tokens\n","unique_words = {word for word in unique_words if word and word not in [\"<EOS>\", \"PAD\"]}\n","\n","# Sorting of unique_words\n","word_vocab = [\"<EOS>\"] + sorted(list(unique_words)) + [\"PAD\"]\n","\n","# Total number of unique words (excluding <EOS> and PAD)\n","total_words = len(word_vocab) - 2\n","\n","# Print the total number of words in the vocabulary\n","print(\"Total number of words in vocabulary (excluding <EOS> and PAD):\", total_words)\n","\n","# Remove words that are used less than a threshold (5 times):\n","threshold = 5\n","filtered_words = {word for word, count in word_counts.items() if count >= threshold}\n","filtered_word_vocab = [\"<EOS>\"] + sorted(list(filtered_words)) + [\"PAD\"]\n","\n","# Number of unique words after filtering (excluding <EOS> and PAD)\n","total_words = len(filtered_word_vocab) - 2\n","\n","# Print the total number of words in the vocabulary\n","print(\"Total number of words in vocabulary after filtering (excluding <EOS> and PAD):\", total_words)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:14.539128Z","iopub.status.busy":"2024-11-25T08:39:14.538874Z","iopub.status.idle":"2024-11-25T08:39:14.556652Z","shell.execute_reply":"2024-11-25T08:39:14.555915Z","shell.execute_reply.started":"2024-11-25T08:39:14.539103Z"},"id":"GWy5SAlnb5NP","trusted":true},"outputs":[],"source":["# Dictionary representing a mapping from words of our word_vocab to integer values\n","word_to_int = {word: i for i, word in enumerate(word_vocab)}\n","\n","assert word_to_int['<EOS>'] == 0 and word_to_int['PAD'] == len(word_vocab) - 1\n","#print(f\"<EOS> index: {word_to_int['<EOS>']}\")\n","#print(f\"PAD index: {word_to_int['PAD']}\")\n","\n","# print(\"Sample mapping:\", list(word_to_int.items())[:10])  # Print first 10 mappings\n","\n","# Dictionary representing the inverse of `word_to_int`, i.e. a mapping from integer (keys) to characters (values).\n","int_to_word = {word:i for i, word in word_to_int.items()}\n","\n","assert int_to_word[0] == '<EOS>' and int_to_word[len(word_vocab)-1] == 'PAD'\n","#print(f\"Word at first index (0): {int_to_word[0]}\")\n","#print(f\"Word at last index ({len(word_vocab)-1}): {int_to_word[len(word_vocab)-1]}\")\n","\n","# print(\"Sample mapping:\", list(int_to_word.items())[:10])  # Print first 10 mappings"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:14.557822Z","iopub.status.busy":"2024-11-25T08:39:14.557535Z","iopub.status.idle":"2024-11-25T08:39:14.567825Z","shell.execute_reply":"2024-11-25T08:39:14.566996Z","shell.execute_reply.started":"2024-11-25T08:39:14.557797Z"},"id":"rh40fQymXcX9","outputId":"30a94506-ded8-4be2-cbb8-d84dd5065179","trusted":true},"outputs":[{"data":{"text/plain":["(0, '<EOS>')"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["word_to_int['<EOS>'],int_to_word[0]"]},{"cell_type":"markdown","metadata":{"id":"ETTonTnlb5NP"},"source":["### Question 5"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:14.569123Z","iopub.status.busy":"2024-11-25T08:39:14.568885Z","iopub.status.idle":"2024-11-25T08:39:14.582740Z","shell.execute_reply":"2024-11-25T08:39:14.581997Z","shell.execute_reply.started":"2024-11-25T08:39:14.569101Z"},"id":"9IQCtHnUb5NP","trusted":true},"outputs":[],"source":["# Create a dataset class\n","# - input:\n","#       list of tokenized sequences\n","#       word_to_int\n","# - Each item: a tuple having\n","#        the indexes of all the words of the sentence except the last one;\n","#        all the elements of that sentence except the first one\n","\n","class Dataset:\n","    def __init__(self, sequences, word_to_int):\n","        self.sequences = sequences\n","        self.word_to_int = word_to_int\n","\n","        # Convert each sequence (list of words) to indexes using map\n","        self.indexed_sequences = [\n","            [self.word_to_int[word] for word in sequence if word in self.word_to_int]\n","            for sequence in self.sequences\n","        ] # the problem is that if in the sequence there is a word (ex '') without mapping, skip it\n","\n","    def __getitem__(self, idx):\n","        # Get the indexed sequence at the given index\n","        indexed_seq = self.indexed_sequences[idx]\n","\n","        # Create x (all indexes except the last one) and y (all indexes except the first one)\n","        x = indexed_seq[:-1]\n","        y = indexed_seq[1:]\n","\n","        return torch.tensor(x), torch.tensor(y)\n","\n","    def __len__(self):\n","        # Return the total number of sequences\n","        return len(self.indexed_sequences)"]},{"cell_type":"markdown","metadata":{"id":"aK54MnTab5NP"},"source":["### Question 6"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:14.584371Z","iopub.status.busy":"2024-11-25T08:39:14.583663Z","iopub.status.idle":"2024-11-25T08:39:14.591848Z","shell.execute_reply":"2024-11-25T08:39:14.591052Z","shell.execute_reply.started":"2024-11-25T08:39:14.584340Z"},"id":"IZendFDTb5NP","trusted":true},"outputs":[],"source":["def collate_fn(batch, pad_value):\n","  # Separate data (x) and target (y) pairs from the batch\n","  data, targets = zip(*batch)\n","\n","  padded_data = nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=pad_value)\n","  padded_targets = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=pad_value)\n","\n","  return padded_data, padded_targets"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:14.592987Z","iopub.status.busy":"2024-11-25T08:39:14.592672Z","iopub.status.idle":"2024-11-25T08:39:14.701443Z","shell.execute_reply":"2024-11-25T08:39:14.700569Z","shell.execute_reply.started":"2024-11-25T08:39:14.592962Z"},"id":"j98SvODmb5NP","trusted":true},"outputs":[],"source":["batch_size = 32\n","\n","dataset = Dataset(ds_train, word_to_int)\n","\n","if batch_size == 1:\n","  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","else:\n","  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n","                          collate_fn=lambda batch: collate_fn(batch, word_to_int[\"PAD\"]))\n","\n","  # By default, DataLoader expects a function like collate_fn(batch) that takes only one argument—the batch itself.\n","  # However, in this case, collate_fn requires an additional argument (pad_value).\n","  # The lambda function allows to rewrite collate_fn(batch, pad_value) into a version compatible with DataLoader"]},{"cell_type":"markdown","metadata":{"id":"yFGFn8FCb5NP"},"source":["## Model definition"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:14.702951Z","iopub.status.busy":"2024-11-25T08:39:14.702636Z","iopub.status.idle":"2024-11-25T08:39:14.709721Z","shell.execute_reply":"2024-11-25T08:39:14.708828Z","shell.execute_reply.started":"2024-11-25T08:39:14.702925Z"},"id":"Y-SnaDznb5NP","trusted":true},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, map, hidden_size=1024, emb_dim=150, n_layers=1):\n","        super(LSTMModel, self).__init__()\n","\n","        self.vocab_size  = len(map)\n","        self.hidden_size = hidden_size\n","        self.emb_dim     = emb_dim\n","        self.n_layers    = n_layers\n","\n","        # Embedding layer\n","        self.embedding = nn.Embedding(\n","            num_embeddings=self.vocab_size,\n","            embedding_dim=self.emb_dim,\n","            padding_idx=map[\"PAD\"]\n","        )\n","\n","        # LSTM layer with potential stacking\n","        self.lstm = nn.LSTM(\n","            input_size=self.emb_dim,\n","            hidden_size=self.hidden_size,\n","            num_layers=self.n_layers,\n","            batch_first=True\n","        )\n","\n","        # Fully connected layer to project LSTM outputs to vocabulary size\n","        self.fc = nn.Linear(\n","            in_features=self.hidden_size,\n","            out_features=self.vocab_size\n","        )\n","\n","    def forward(self, x, prev_state):\n","        # Embedding lookup for input tokens\n","        embed = self.embedding(x)\n","\n","        # Pass embeddings through the LSTM\n","        yhat, state = self.lstm(embed, prev_state)  # yhat: (batch, seq_length, hidden_size)\n","\n","        # Pass through the fully connected layer to get logits\n","        out = self.fc(yhat)  # out: (batch, seq_length, vocab_size)\n","\n","        return out, state\n","\n","    def init_state(self, b_size=1):\n","        # Initializes hidden and cell states with zeros\n","        # Each state has shape (n_layers, batch_size, hidden_size)\n","        return (torch.zeros(self.n_layers, b_size, self.hidden_size),\n","                torch.zeros(self.n_layers, b_size, self.hidden_size))"]},{"cell_type":"markdown","metadata":{"id":"Gkxgye6Hb5NQ"},"source":["## Evaluation 1"]},{"cell_type":"markdown","metadata":{"id":"czzpIlYgb5NQ"},"source":["$random\\_sample\\_next$ which randomly sample the next word on $p(wn|w0, w1, . . . , wn−1)$"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:14.711222Z","iopub.status.busy":"2024-11-25T08:39:14.710906Z","iopub.status.idle":"2024-11-25T08:39:14.722973Z","shell.execute_reply":"2024-11-25T08:39:14.722227Z","shell.execute_reply.started":"2024-11-25T08:39:14.711186Z"},"id":"C3tTRb7jb5NQ","trusted":true},"outputs":[],"source":["\n","def random_sample_next(model, x, prev_state, topk=None):\n","    \"\"\"\n","    Randomly samples the next word based on the probability distribution.\n","\n","    Args:\n","        model: LSTM model.\n","        x: Input tensor of shape (batch_size, seq_length).\n","        prev_state: Previous hidden and cell state of the LSTM.\n","        topk: Number of top candidates to consider for sampling. Defaults to all if None.\n","\n","    Returns:\n","        sampled_ix: Index of the randomly sampled word.\n","        state: Updated LSTM state after processing the input.\n","    \"\"\"\n","    # Perform forward-prop and get the output of the last time-step\n","    out, state = model(x, prev_state)\n","    last_out = out[0, -1, :]  # Vocabulary values of last element of sequence\n","\n","    # if topk is not None:\n","\n","    # Get the top-k indexes and their values\n","    topk = topk if topk else last_out.shape[0]\n","\n","    top_logit, top_ix = torch.topk(last_out, k=topk, dim=-1)\n","\n","    # Convert logits to probabilities and sample\n","    p = F.softmax(top_logit.detach(), dim=-1).cpu().numpy() # Move to CPU before converting to numpy\n","    top_ix = top_ix.cpu().numpy()  # Move to CPU before converting to numpy\n","\n","    # Check if top_ix is empty\n","    if len(top_ix) == 0:\n","        raise ValueError(\"No valid predictions were made (top_ix is empty).\")\n","\n","    sampled_ix = np.random.choice(top_ix, p=p)\n","\n","    return sampled_ix, state"]},{"cell_type":"markdown","metadata":{"id":"LWuKKum2b5NQ"},"source":["$sample\\_argmax$ which picks the word having the highest probability according to the distribution $p(wn|w0, w1, . . . , wn−1)$."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:14.724524Z","iopub.status.busy":"2024-11-25T08:39:14.724195Z","iopub.status.idle":"2024-11-25T08:39:14.735509Z","shell.execute_reply":"2024-11-25T08:39:14.734856Z","shell.execute_reply.started":"2024-11-25T08:39:14.724489Z"},"id":"8OsamFaQb5NQ","trusted":true},"outputs":[],"source":["def sample_argmax(model, x, prev_state):\n","    \"\"\"\n","    Samples the next word by picking the one with the highest probability (argmax strategy).\n","\n","    Args:\n","        model: Trained LSTM model.\n","        x: Input tensor of shape (batch_size, seq_length).\n","        prev_state: Previous hidden and cell state of the LSTM.\n","\n","    Returns:\n","        sampled_ix: Index of the word with the highest probability.\n","        state: Updated LSTM state after processing the input.\n","    \"\"\"\n","    # Perform forward-prop and get the output of the last time-step\n","    out, state = model(x, prev_state)\n","    last_out = out[0, -1, :]  # Vocabulary values of last element of sequence\n","\n","    # Get the index with the highest probability\n","    sampled_ix = torch.argmax(last_out).item()\n","\n","    return sampled_ix, state"]},{"cell_type":"markdown","metadata":{"id":"S1DSHZJGb5NQ"},"source":["$sample$ takes as minimal input: a prompt (some words), the model, and one of the two functions above defined"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:14.737437Z","iopub.status.busy":"2024-11-25T08:39:14.736612Z","iopub.status.idle":"2024-11-25T08:39:14.751780Z","shell.execute_reply":"2024-11-25T08:39:14.750997Z","shell.execute_reply.started":"2024-11-25T08:39:14.737401Z"},"id":"pwbrxSjNb5NQ","trusted":true},"outputs":[],"source":["def sample(model, seed, stop_on, strategy=\"random\", topk=5, max_seqlen=18):\n","    \"\"\"\n","    Generates a sequence using the model.\n","\n","    Args:\n","        model: Trained LSTM model.\n","        seed: Initial list of token indices to start generation.\n","        strategy: Sampling strategy - 'random' or 'max'.\n","        topk: Number of top candidates to consider for 'random' sampling.\n","        max_seqlen: Maximum sequence length to generate.\n","        stop_on: Token index to stop generation.\n","\n","    Returns:\n","        sampled_ix_list: List of token indices for the generated sequence.\n","    \"\"\"\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps'\n","        if torch.backends.mps.is_available() else 'cpu')\n","\n","    # the model expect that seed (prompt) is a list or a tuple to iter on it. But if it is a single int transform it in the correct form\n","    seed = seed if isinstance(seed, (list, tuple)) else [seed]\n","    model.eval()\n","    with torch.no_grad():\n","        sampled_ix_list = seed[:]\n","        x = torch.tensor([seed], device=DEVICE)\n","\n","        prev_state = model.init_state(b_size=x.shape[0])\n","\n","        # in LSTM prev_state is a tuple\n","        prev_state = tuple(s.to(DEVICE) for s in prev_state)\n","\n","        for _ in range(max_seqlen - len(seed)):\n","            # Repeatedly predicts the next word/token based on the input sequence\n","            if strategy == \"random\":\n","                sampled_ix, prev_state = random_sample_next(model, x, prev_state, topk)\n","            elif strategy == \"argmax\":\n","                sampled_ix, prev_state = sample_argmax(model, x, prev_state)\n","            else:\n","                raise ValueError(f\"Invalid sampling strategy: {strategy}\")\n","\n","            # The predicted token is appended to the sequence\n","            sampled_ix_list.append(sampled_ix)\n","\n","            # The new token is used as the input for the next prediction\n","            x = torch.tensor([[sampled_ix]], device=DEVICE)\n","\n","            # If the predicted token is word_to_int[\"<EOS>\"] the function terminates the loop\n","            if sampled_ix == stop_on:\n","              break\n","\n","    model.train()\n","    return sampled_ix_list"]},{"cell_type":"markdown","metadata":{"id":"5vrVcYjbb5NQ"},"source":["Prior to training, evaluate your model generating and reporting here 2/3 sentences in the following way"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:14.752914Z","iopub.status.busy":"2024-11-25T08:39:14.752655Z","iopub.status.idle":"2024-11-25T08:39:15.187045Z","shell.execute_reply":"2024-11-25T08:39:15.186096Z","shell.execute_reply.started":"2024-11-25T08:39:14.752892Z"},"id":"_IP4vRotb5NQ","trusted":true},"outputs":[],"source":["model = LSTMModel(word_to_int)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:15.188407Z","iopub.status.busy":"2024-11-25T08:39:15.188132Z","iopub.status.idle":"2024-11-25T08:39:15.542377Z","shell.execute_reply":"2024-11-25T08:39:15.541517Z","shell.execute_reply.started":"2024-11-25T08:39:15.188381Z"},"id":"NGHp7mi6cCBU","outputId":"5d11e5f6-5fd3-4616-956a-ee7c5ae54353","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Working on cuda\n"]}],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps'\n","        if torch.backends.mps.is_available() else 'cpu')\n","model = model.to(DEVICE)\n","print(\"Working on\", DEVICE)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-11-25T08:39:15.544040Z","iopub.status.busy":"2024-11-25T08:39:15.543656Z","iopub.status.idle":"2024-11-25T08:39:15.549566Z","shell.execute_reply":"2024-11-25T08:39:15.548606Z","shell.execute_reply.started":"2024-11-25T08:39:15.544001Z"},"id":"e5Xyt96Pb5NQ","trusted":true},"outputs":[],"source":["# Function to tokenize and map words to indices\n","def tokenize_and_map(sentence, word_to_int):\n","    tokens = sentence.split(\" \")  # Split sentence into words\n","    return [word_to_int[word] for word in tokens if word in word_to_int]\n","\n","def decode_sequence(generated_tokens, int_to_word):\n","    # Convert a list of token IDs into words using the int_to_word mapping\n","    return [int_to_word[token] for token in generated_tokens if token in int_to_word]"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:15.550994Z","iopub.status.busy":"2024-11-25T08:39:15.550739Z","iopub.status.idle":"2024-11-25T08:39:15.948610Z","shell.execute_reply":"2024-11-25T08:39:15.947736Z","shell.execute_reply.started":"2024-11-25T08:39:15.550970Z"},"id":"RLs3gOWab5NR","outputId":"7269496f-c289-4457-e963-966da7c65c37","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Seed: [29513, 23553, 31678]\n","Generated:  ['the', 'president', 'wants', \"'texas\", \"vikings'\", 'ashton', '2.0', 'graham:', \"'linked\", 'undecided', 'dominant', \"grandparent's\", \"'breaking\", 'merely', 'crook;', 'sprinting', 'church-state', 'story:']\n","18\n","Generated:  ['the', 'president', 'wants', 'damon', 'marking', 'deal', 'prophetic', 'scratch', 'marking', 'cross', 'vaunted', 'palestine:', 'poppy', 'lenin', 'torpedo', '38th', '‘working', 'confidant']\n","18\n","Generated:  ['the', 'president', 'wants', 'groped', 'force:', 'president', 'pigs', 'hospital', 'heart?', \"'mythology'\", 'tease:', 'ballooned', 'soars,', 'trump-abandoning', 'partnership,', 'reasonable,', 'unmasked', '31,']\n","18\n"]}],"source":["# Start with any prompt, e.g., “the president wants”, and generate three sentences with the sampling strategy.\n","seed = \"the president wants\"\n","seed = tokenize_and_map(seed, word_to_int)\n","print(f\"Seed: {seed}\")\n","\n","for i in range(3):\n","    generated = sample(model, seed, word_to_int[\"<EOS>\"], \"random\")\n","    generated = decode_sequence(generated, int_to_word)\n","    print(\"Generated: \", generated)\n","    print(len(generated))"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-25T08:39:15.950504Z","iopub.status.busy":"2024-11-25T08:39:15.950109Z","iopub.status.idle":"2024-11-25T08:39:16.017938Z","shell.execute_reply":"2024-11-25T08:39:16.017150Z","shell.execute_reply.started":"2024-11-25T08:39:15.950461Z"},"id":"tYLPZjBYb5NR","outputId":"ab6b69ba-234e-427d-fab4-c008182da3f5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Seed: [29513, 23553, 31678]\n","Generated:  ['the', 'president', 'wants', 'groped', 'berhard', \"'complete\", 'superheroes', 'malnourished', 'theater', 'coulter', 'relatives', 'if)', \"pac's\", 'returned', 'love', \"5-year-old'\", 'dinner,', 'online']\n","18\n","Generated:  ['the', 'president', 'wants', 'groped', 'berhard', \"'complete\", 'superheroes', 'malnourished', 'theater', 'coulter', 'relatives', 'if)', \"pac's\", 'returned', 'love', \"5-year-old'\", 'dinner,', 'online']\n","18\n","Generated:  ['the', 'president', 'wants', 'groped', 'berhard', \"'complete\", 'superheroes', 'malnourished', 'theater', 'coulter', 'relatives', 'if)', \"pac's\", 'returned', 'love', \"5-year-old'\", 'dinner,', 'online']\n","18\n"]}],"source":["# Start with any prompt, e.g., “the president wants”, and generate three sentences with the sampling strategy.\n","seed = \"the president wants\"\n","seed = tokenize_and_map(seed, word_to_int)  # Convert to token indices\n","print(f\"Seed: {seed}\")\n","\n","for i in range(3):\n","    generated = sample(model, seed, word_to_int[\"<EOS>\"], \"argmax\")\n","    generated = decode_sequence(generated, int_to_word)\n","    print(\"Generated: \", generated)\n","    print(len(generated))"]},{"cell_type":"markdown","metadata":{"id":"I4apAckjb5NR"},"source":["## Training"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"execution":{"iopub.execute_input":"2024-11-25T08:42:11.490322Z","iopub.status.busy":"2024-11-25T08:42:11.489966Z","iopub.status.idle":"2024-11-25T08:42:11.502019Z","shell.execute_reply":"2024-11-25T08:42:11.501268Z","shell.execute_reply.started":"2024-11-25T08:42:11.490294Z"},"id":"y3aJQBwMb5NR","outputId":"fece5bc7-a204-4b29-b55e-3a97e116c5c6","trusted":true},"outputs":[],"source":["def train(model, data, num_epochs, criterion, lr=0.001, print_every=50, clip=None):\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(DEVICE)\n","    criterion = criterion.to(DEVICE)\n","    model.train()\n","    \n","    costs = []\n","    running_loss = 0\n","    loss_hist = []\n","    generated_text_list = []\n","    perplexity_hist = []\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    \n","    total_batches = len(data)\n","    epoch = 0\n","    while epoch < num_epochs:\n","        epoch += 1\n","        epoch_loss = 0.0\n","        \n","        for batch_idx, (x, y) in enumerate(data, 1):\n","            x = x.to(DEVICE)\n","            y = y.to(DEVICE)\n","            optimizer.zero_grad()\n","            \n","            # Initialize hidden state\n","            prev_state = model.init_state(b_size=x.shape[0])\n","            prev_state = tuple(s.to(DEVICE) for s in prev_state)\n","            \n","            # Forward pass\n","            out, state = model(x, prev_state=prev_state)\n","            \n","            # Reshape output for CrossEntropyLoss [batch_size, vocab_size, sequence_length]\n","            loss_out = out.permute(0, 2, 1)\n","            \n","            # Calculate loss\n","            loss = criterion(loss_out, y)\n","            epoch_loss += loss.item()\n","            costs.append(loss.item())\n","            running_loss += loss.item()\n","            \n","            # Backward pass and optimization\n","            loss.backward()\n","            if clip:\n","                nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","        \n","        # Calculate average loss for the epoch\n","        avg_epoch_loss = epoch_loss / total_batches\n","        loss_hist.append(avg_epoch_loss)\n","        \n","        # Calculate perplexity directly from cross-entropy loss\n","        perplexity = torch.exp(torch.tensor(avg_epoch_loss))\n","        perplexity_hist.append(perplexity.item())\n","        \n","        if print_every and (epoch % print_every) == 0:\n","            print(f\"Epoch: {epoch}/{num_epochs}, Loss: {avg_epoch_loss:8.4f}, Perplexity: {perplexity:8.4f}\")\n","            generated_indices = sample(model, seed, word_to_int[\"<EOS>\"], \"argmax\")\n","            generated = decode_sequence(generated_indices, int_to_word)\n","            print(f\"Generated text: {generated}\\n\")\n","            \n","        # Early stopping check\n","        if avg_epoch_loss < 1.5:\n","            print(f\"\\nTarget loss of 1.5 reached at epoch {epoch}!\")\n","            break\n","        \n","    if len(generated_text_list) > 0:\n","        print(\"Beginning list:\", generated_text_list[0])\n","        middle_index = len(generated_text_list) // 2\n","        print(\"Middle list:\", generated_text_list[middle_index])\n","        print(\"End list:\", generated_text_list[-1])\n","        \n","    return model, costs, loss_hist, perplexity_hist"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"execution":{"iopub.execute_input":"2024-11-25T08:42:14.347291Z","iopub.status.busy":"2024-11-25T08:42:14.346959Z","iopub.status.idle":"2024-11-25T08:48:50.556710Z","shell.execute_reply":"2024-11-25T08:48:50.555834Z","shell.execute_reply.started":"2024-11-25T08:42:14.347262Z"},"id":"y3aJQBwMb5NR","outputId":"fece5bc7-a204-4b29-b55e-3a97e116c5c6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/12, Loss:   5.7838, Perplexity: 324.9933\n","Generated text: ['the', 'president', 'wants', 'to', 'make', 'the', 'first', 'latina', 'in', 'the', 'white', 'house', '<EOS>']\n","\n","Epoch: 2/12, Loss:   4.4944, Perplexity:  89.5178\n","Generated text: ['the', 'president', 'wants', 'to', 'stop', 'donald', 'trump', '<EOS>']\n","\n","Epoch: 3/12, Loss:   3.1552, Perplexity:  23.4577\n","Generated text: ['the', 'president', 'wants', 'to', 'bring', 'back', 'the', \"'90s\", 'economy.', \"here's\", 'what', 'it', 'deserves', '<EOS>']\n","\n","Epoch: 4/12, Loss:   2.2588, Perplexity:   9.5718\n","Generated text: ['the', 'president', 'wants', 'to', 'take', 'back', 'the', 'house?', '<EOS>']\n","\n","Epoch: 5/12, Loss:   1.6364, Perplexity:   5.1366\n","Generated text: ['the', 'president', 'wants', 'to', 'come', 'out', 'the', 'middle', 'class', '<EOS>']\n","\n","Epoch: 6/12, Loss:   1.2286, Perplexity:   3.4165\n","Generated text: ['the', 'president', 'wants', 'to', 'take', 'the', 'naturalization', 'act', 'of', 'corruption', '<EOS>']\n","\n","\n","Target loss of 1.5 reached at epoch 6!\n"]}],"source":["criterion = nn.CrossEntropyLoss(ignore_index=word_to_int[\"PAD\"])\n","model, costs, loss_hist, perplexity_hist = train(model, dataloader, 12, criterion, lr=1e-3,\n","                                 print_every=1, clip=1)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"deeple","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"}},"nbformat":4,"nbformat_minor":4}
