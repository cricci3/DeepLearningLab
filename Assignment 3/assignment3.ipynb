{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "# Probably, this below must be changed if you work with a M1/M2/M3 Mac\n",
    "torch.cuda.manual_seed(seed) # for CUDA\n",
    "torch.backends.cudnn.deterministic = True # for CUDNN\n",
    "torch.backends.benchmark = False # if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['link', 'headline', 'category', 'short_description', 'authors', 'date'],\n",
      "    num_rows: 209527\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "ds = load_dataset(\"heegyu/news-category-dataset\")\n",
    "print(ds['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First headline (before processing): Biden Says U.S. Forces Would Defend Taiwan If China Invaded\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "# Filter for \"POLITICS\" category and store each headline as a string in ds_train\n",
    "ds_train = [news['headline'] for news in ds['train'] if news['category'] == 'POLITICS']\n",
    "\n",
    "assert len(ds_train) == 35602\n",
    "\n",
    "print(\"First headline (before processing):\", ds_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biden says u.s. forces would defend taiwan if china invaded\n",
      "['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded']\n"
     ]
    }
   ],
   "source": [
    "# Convert each headline to lowercase\n",
    "ds_train = [headline.lower() for headline in ds_train]\n",
    "\n",
    "# Check the result\n",
    "print(ds_train[0])\n",
    "\n",
    "# Split each headline in words\n",
    "# maybe I could use a better tokenizer (ex. remove all punctation)\n",
    "ds_train = [headline.split(\" \") for headline in ds_train]\n",
    "\n",
    "# Check the result\n",
    "print(ds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Add <EOS> at the end of every headline\n",
    "for headline in ds_train:\n",
    "    headline.append('<EOS>')\n",
    "\n",
    "# Check the result\n",
    "print(ds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['biden', 'says', 'u.s.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded', '<EOS>'], ['‘beautiful', 'and', 'sad', 'at', 'the', 'same', 'time’:', 'ukrainian', 'cultural', 'festival', 'takes', 'on', 'a', 'deeper', 'meaning', 'this', 'year', '<EOS>'], ['biden', 'says', \"queen's\", 'death', 'left', \"'giant\", \"hole'\", 'for', 'royal', 'family', '<EOS>'], ['bill', 'to', 'help', 'afghans', 'who', 'escaped', 'taliban', 'faces', 'long', 'odds', 'in', 'the', 'senate', '<EOS>'], ['mark', 'meadows', 'complies', 'with', 'justice', 'dept.', 'subpoena:', 'report', '<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print(ds_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words: [('<EOS>', 35602), ('to', 10701), ('the', 9618), ('trump', 6895), ('of', 5536)]\n"
     ]
    }
   ],
   "source": [
    "# Flatten ds_train and extract all words (including <EOS> and PAD tokens)\n",
    "all_words = [word for headline in ds_train for word in headline]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Get the 5 most common words\n",
    "most_common_words = word_counts.most_common(5)\n",
    "\n",
    "# Print the 5 most common words\n",
    "print(\"5 most common words:\", most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in vocabulary (excluding <EOS> and PAD): 33230\n",
      "Total number of words in vocabulary after filtering (excluding <EOS> and PAD): 7034\n"
     ]
    }
   ],
   "source": [
    "# Flatten ds_train and extract unique words \n",
    "unique_words = set(word for headline in ds_train for word in headline)\n",
    "\n",
    "# Create vocabulary with <EOS> at the beginning and PAD at the end and remove evenutally alredy presents special tokens\n",
    "unique_words = {word for word in unique_words if word and word not in [\"<EOS>\", \"PAD\"]}\n",
    "\n",
    "# Sorting of unique_words\n",
    "word_vocab = [\"<EOS>\"] + sorted(list(unique_words)) + [\"PAD\"]\n",
    "\n",
    "# Total number of unique words (excluding <EOS> and PAD)\n",
    "total_words = len(word_vocab) - 2\n",
    "\n",
    "# Print the total number of words in the vocabulary\n",
    "print(\"Total number of words in vocabulary (excluding <EOS> and PAD):\", total_words)\n",
    "\n",
    "# Remove words that are used less than a threshold (5 times):\n",
    "threshold = 5\n",
    "filtered_words = {word for word, count in word_counts.items() if count >= threshold}\n",
    "filtered_word_vocab = [\"<EOS>\"] + sorted(list(filtered_words)) + [\"PAD\"]\n",
    "\n",
    "# Number of unique words after filtering (excluding <EOS> and PAD)\n",
    "total_words = len(filtered_word_vocab) - 2\n",
    "\n",
    "# Print the total number of words in the vocabulary\n",
    "print(\"Total number of words in vocabulary after filtering (excluding <EOS> and PAD):\", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary representing a mapping from words of our word_vocab to integer values\n",
    "word_to_int = {word: i for i, word in enumerate(word_vocab)}\n",
    "\n",
    "assert word_to_int['<EOS>'] == 0 and word_to_int['PAD'] == len(word_vocab) - 1\n",
    "#print(f\"<EOS> index: {word_to_int['<EOS>']}\")\n",
    "#print(f\"PAD index: {word_to_int['PAD']}\")\n",
    "\n",
    "# print(\"Sample mapping:\", list(word_to_int.items())[:10])  # Print first 10 mappings\n",
    "\n",
    "# Dictionary representing the inverse of `word_to_int`, i.e. a mapping from integer (keys) to characters (values).\n",
    "int_to_word = {word:i for i, word in word_to_int.items()}\n",
    "\n",
    "assert int_to_word[0] == '<EOS>' and int_to_word[len(word_vocab)-1] == 'PAD'\n",
    "#print(f\"Word at first index (0): {int_to_word[0]}\")\n",
    "#print(f\"Word at last index ({len(word_vocab)-1}): {int_to_word[len(word_vocab)-1]}\")\n",
    "\n",
    "# print(\"Sample mapping:\", list(int_to_word.items())[:10])  # Print first 10 mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset class\n",
    "# - input:\n",
    "#       list of tokenized sequences\n",
    "#       word_to_int\n",
    "# - Each item: a tuple having\n",
    "#        the indexes of all the words of the sentence except the last one;\n",
    "#        all the elements of that sentence except the first one\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, sequences, word_to_int):\n",
    "        self.sequences = sequences\n",
    "        self.word_to_int = word_to_int\n",
    "\n",
    "        # Convert each sequence (list of words) to indexes using map\n",
    "        self.indexed_sequences = [\n",
    "            [self.word_to_int[word] for word in sequence if word in self.word_to_int] \n",
    "            for sequence in self.sequences\n",
    "        ] # the problem is that if in the sequence there is a word (ex '') without mapping, skip it\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the indexed sequence at the given index\n",
    "        indexed_seq = self.indexed_sequences[idx]\n",
    "        \n",
    "        # Create x (all indexes except the last one) and y (all indexes except the first one)\n",
    "        x = indexed_seq[:-1]\n",
    "        y = indexed_seq[1:]\n",
    "        \n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the total number of sequences\n",
    "        return len(self.indexed_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_value):\n",
    "  # Separate data (x) and target (y) pairs from the batch\n",
    "  data, targets = zip(*batch)\n",
    "\n",
    "  padded_data = nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=pad_value)\n",
    "  padded_targets = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "  return padded_data, padded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = Dataset(ds_train, word_to_int)\n",
    "\n",
    "if batch_size == 1:\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, word_to_int[\"PAD\"]))\n",
    "  \n",
    "  # By default, DataLoader expects a function like collate_fn(batch) that takes only one argument—the batch itself.\n",
    "  # However, in this case, collate_fn requires an additional argument (pad_value).\n",
    "  # The lambda function allows to rewrite collate_fn(batch, pad_value) into a version compatible with DataLoader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
