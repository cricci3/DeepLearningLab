{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# *** Packages ***\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torchvision import transforms\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from math import floor"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set the seed for reproducibility\n","manual_seed = 42\n","torch.manual_seed(manual_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["'''\n","Q2\n","'''\n","batch_size = 32\n","\n","transformer = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0,0,0), std=(1,1,1))])\n","\n","dataset_train= datasets.CIFAR10(root='./data', train=True, download=True, transform=transformer)#transforms.ToTensor())\n","trainloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","\n","dataset_test= datasets.CIFAR10(root='./data', train=False, download=True, transform=transformer)\n","#testloader = DataLoader(dataset_test, batch_size=len(dataset_test))\n","testloader = DataLoader(dataset_test, batch_size=batch_size)\n","\n","\n","classes_map = {\n","    0 : 'plane',\n","    1 : 'car',\n","    2 : 'bird',\n","    3 : 'cat',\n","    4 : 'deer',\n","    5 : 'dog',\n","    6 : 'frog',\n","    7 : 'horse',\n","    8 : 'ship',\n","    9 : 'truck'\n","}\n","\n","figure = plt.figure(figsize=(8, 4))\n","cols, rows = 5, 2\n","\n","# Creiamo un dizionario per tenere traccia delle immagini trovate per ogni classe\n","class_examples = {}\n","\n","# Cerchiamo un esempio per ogni classe\n","for i, (img, label) in enumerate(dataset_train):\n","        # dataset return img, label \n","        # enumerate return i\n","    if label not in class_examples and len(class_examples) < 10:\n","        class_examples[label] = img\n","    if len(class_examples) == 10:\n","        break\n","\n","# Plottiamo un'immagine per ogni classe\n","for i, (label, img) in enumerate(class_examples.items(), 1):\n","    figure.add_subplot(rows, cols, i)\n","    img = img.permute(1, 2, 0)\n","    plt.title(classes_map[label])\n","    plt.axis(\"off\")\n","    plt.imshow(img)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# Funzione per estrarre tutte le etichette dal dataloader\n","def get_labels_from_loader(loader):\n","    all_labels = []\n","    for _, labels in loader:\n","        all_labels.extend(labels.numpy())  # Estraiamo le etichette come array numpy\n","    return np.array(all_labels)\n","\n","\n","# Otteniamo le etichette da train e test e put them into an array\n","train_label = get_labels_from_loader(trainloader)\n","test_label = get_labels_from_loader(testloader)\n","\n","# Calcoliamo la distribuzione delle classi\n","_, train_counts = np.unique(train_label, return_counts=True)\n","_, test_counts = np.unique(test_label, return_counts=True)\n","\n","# Creiamo il grafico comparativo\n","bar_width = 0.35\n","index = np.arange(len(class_examples))\n","\n","plt.bar(index, train_counts, bar_width, label='Train', color='deepskyblue')\n","plt.bar(index + bar_width, test_counts, bar_width, label='Test', color='lightgreen')\n","\n","plt.xlabel('Classes')\n","plt.ylabel('Number of Images')\n","plt.title('Class Distribution in Train and Test Sets')\n","plt.xticks(index + bar_width / 2, class_examples)\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# Stampiamo i valori\n","print(\"\\nNumero di immagini per classe:\")\n","print(f\"{'Classe':<10} {'Training':<10} {'Test':<10}\")\n","print(\"-\" * 30)\n","for i in range(10):\n","    print(f\"{classes_map[i]:<10} {train_counts[i]:<10} {test_counts[i]:<10}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["'''\n","Q3\n","'''\n","trainiter = iter(trainloader)\n","train_images, _ = next(trainiter)\n","\n","testiter = iter(testloader)\n","test_images, _ = next(testiter)\n","\n","\n","# Prendi la prima immagine del batch\n","first_image = train_images[0]\n","\n","print(f\"Type of each image: {first_image.type}\")\n","\n","# Stampa la dimensione dell'immagine\n","print(f\"Shape of the image tensor: {first_image.shape}\")  # (C, H, W)\n","\n","# Puoi separare in Channels, Height, Width\n","channels, height, width = first_image.shape\n","print(f\"Width: {width}, Height: {height}, Channels: {channels}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["'''\n","Q5\n","'''\n","dataset_val, dataset_test = torch.utils.data.random_split(dataset_test, [0.5, 0.5])\n","\n","#validloader = DataLoader(dataset_val, batch_size=len(dataset_val))\n","testloader = DataLoader(dataset_test, batch_size=len(dataset_test))\n","validloader = DataLoader(dataset_val, batch_size=batch_size)\n","\n","print(len(trainloader), len(validloader))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["'''\n","Q6\n","'''\n","def out_dimensions(conv_layer, h_in, w_in):\n","    '''\n","    This function computes the output dimension of each convolutional layers in the most general way. \n","    '''\n","    h_out = floor((h_in + 2 * conv_layer.padding[0] - conv_layer.dilation[0] * (conv_layer.kernel_size[0] - 1) - 1) /\n","                  conv_layer.stride[0] + 1)\n","    w_out = floor((w_in + 2 * conv_layer.padding[1] - conv_layer.dilation[1] * (conv_layer.kernel_size[1] - 1) - 1) /\n","                  conv_layer.stride[1] + 1)\n","    return h_out, w_out\n","    \n","class CNNB(nn.Module):\n","    def __init__(self):\n","        super(CNNB, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), padding=0, stride=1)\n","        h_out, w_out = out_dimensions(self.conv1, 32, 32)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=0, stride=1)\n","        h_out, w_out = out_dimensions(self.conv2, h_out, w_out)\n","        self.relu1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        h_out, w_out = int(h_out/2), int(w_out/2)\n","        \n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=0, stride=1)\n","        h_out, w_out = out_dimensions(self.conv3, h_out, w_out)\n","        self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3, 3), padding=0, stride=1)\n","        h_out, w_out = out_dimensions(self.conv4, h_out, w_out)\n","        h_out, w_out = int(h_out/2), int(w_out/2)\n","        \n","        self.dimensions_final = (64, h_out, w_out)\n","        self.fc1 = nn.Linear(64 * h_out * w_out, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 10)\n","\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.relu1(x)\n","        x = self.pool1(x)\n","        \n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.relu1(x)\n","        x = self.pool1(x)\n","        \n","        n_channels, h, w = self.dimensions_final\n","        x = x.view(-1, n_channels * h * w)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["    \n","'''\n","Q7\n","'''\n","model = CNNB()\n","learning_rate = 0.031\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n","    if torch.backends.mps.is_available() else 'cpu')\n","model = model.to(DEVICE)\n","print(\"Working on\", DEVICE)\n","\n","\n","n_epochs = 4\n","n_steps = 50\n","\n","train_acc_list, eval_acc_list = [], []\n","train_loss_list, validation_loss_list = [], []\n","train_loss_list_epochs, eval_loss_list_epochs = [], []\n","\n","for epoch in range(n_epochs):\n","    # Reset training counters for each epoch\n","    n_samples_train, n_correct_train = 0, 0\n","    loss_train = 0\n","\n","    for step, (data, target) in enumerate(trainloader):\n","        model.train()\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","\n","        optimizer.zero_grad()\n","        output = model(data)\n","        _, predicted = torch.max(output.data, 1)\n","\n","        # Update counts\n","        n_samples_train += target.size(0)\n","        n_correct_train += (predicted == target).sum().item()\n","\n","        # Compute and accumulate loss\n","        loss = loss_fn(output, target)\n","        loss_train += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Log training metrics every n_steps\n","        if (step + 1) % n_steps == 0:\n","            acc_train = 100.0 * n_correct_train / n_samples_train\n","            avg_loss_train = loss_train / (step + 1)  # Average loss up to current step\n","            train_loss_list.append(avg_loss_train)\n","            train_acc_list.append(acc_train)\n","            print(f\"Epoch [{epoch+1}/{n_epochs}], Step [{step+1}/{len(trainloader)}]\")\n","            print(f\"Training Accuracy: {acc_train:.2f}%\")\n","            print(f\"Training Loss: {avg_loss_train:.4f}\")\n","\n","    # Save the last computed training loss of the epoch\n","    last_loss_train = loss_train / len(trainloader)  # Final averaged loss for the epoch\n","    train_loss_list_epochs.append(last_loss_train)\n","    \n","    # Validation phase\n","    model.eval()\n","    n_samples_eval, n_correct_eval = 0, 0\n","    loss_eval = 0\n","    \n","    with torch.no_grad():\n","        for step, (data, target) in enumerate(validloader):\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            output = model(data)\n","            _, predicted = torch.max(output.data, 1)\n","\n","            # Update evaluation counts\n","            n_samples_eval += target.size(0)\n","            n_correct_eval += (predicted == target).sum().item()\n","            \n","            # Compute and accumulate validation loss\n","            loss = loss_fn(output, target)\n","            loss_eval += loss.item()\n","\n","            # Log validation metrics every n_steps\n","            if (step + 1) % n_steps == 0:\n","                acc_eval = 100.0 * n_correct_eval / n_samples_eval\n","                avg_loss_eval = loss_eval / (step + 1)  # Average loss up to current step\n","                validation_loss_list.append(avg_loss_eval)\n","                eval_acc_list.append(acc_eval)\n","                print(f\"Step [{step+1}/{len(validloader)}]\")\n","                print(f\"Validation Accuracy: {acc_eval:.2f}%\")\n","                print(f\"Validation Loss: {avg_loss_eval:.4f}\")\n","        \n","        # Save the last computed validation loss of the epoch\n","        last_loss_eval = loss_eval / len(validloader)  # Final averaged loss for the epoch\n","        eval_loss_list_epochs.append(last_loss_eval)\n","        acc_eval = 100.0 * n_correct_eval / n_samples_eval\n","        print(f\"Epoch [{epoch+1}/{n_epochs}], Validation Loss: {last_loss_eval:.4f}, Validation Accuracy: {acc_eval:.2f}%\")\n","\n","\n","# test\n","with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    for data, target in testloader:\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        outputs = model(data)\n","        _, predicted = torch.max(outputs.data, 1)\n","        n_samples += target.size(0)\n","        n_correct += (predicted == target).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","print(\"Accuracy on the test set:\", acc, \"%\")\n","\n","print(len(train_loss_list), len(validation_loss_list))\n","print(len(train_loss_list_epochs), len(eval_loss_list_epochs))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["'''\n","Q8\n","'''\n","plt.figure()\n","plt.plot(range(n_epochs), train_loss_list_epochs)\n","plt.plot(range(n_epochs), eval_loss_list_epochs)\n","plt.legend([\"Train loss\", \"Validation Loss\"])\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss value\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["'''\n","Q9\n","'''\n","class CNNGodzilla(nn.Module):\n","    def __init__(self):\n","        super(CNNGodzilla, self).__init__()\n","        \n","        # First block\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), padding=1, stride=1)\n","        h_out, w_out = out_dimensions(self.conv1, 32, 32)  # 32x32\n","        self.BN1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1, stride=1)\n","        h_out, w_out = out_dimensions(self.conv2, h_out, w_out)  # 32x32\n","        self.BN2 = nn.BatchNorm2d(32)\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        h_out, w_out = int(h_out/2), int(w_out/2)  # 16x16\n","        \n","        # Second block\n","        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1, stride=1)\n","        h_out, w_out = out_dimensions(self.conv3, h_out, w_out)  # 16x16\n","        self.BN3 = nn.BatchNorm2d(64)\n","        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1, stride=1)\n","        h_out, w_out = out_dimensions(self.conv4, h_out, w_out)  # 16x16\n","        self.BN4 = nn.BatchNorm2d(64)\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        h_out, w_out = int(h_out/2), int(w_out/2)  # 8x8\n","        \n","        # Third block\n","        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1, stride=1)\n","        h_out, w_out = out_dimensions(self.conv5, h_out, w_out)  # 8x8\n","        self.BN5 = nn.BatchNorm2d(128)\n","        self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding=1, stride=1)\n","        h_out, w_out = out_dimensions(self.conv6, h_out, w_out)  # 8x8\n","        self.BN6 = nn.BatchNorm2d(256)\n","        self.pool3 = nn.MaxPool2d(2, 2)\n","        h_out, w_out = int(h_out/2), int(w_out/2)  # 4x4\n","        \n","        # Flatten\n","        self.flatten = nn.Flatten()\n","        \n","        # Store final dimensions for the forward pass\n","        self.dimensions_final = (256, h_out, w_out)  # Should be (256, 4, 4)\n","        \n","        # Fully Connected\n","        self.fc1 = nn.Linear(256 * h_out * w_out, 128)  # 256 * 4 * 4 = 4096 input features\n","        self.BN7 = nn.BatchNorm1d(128)\n","        self.Dropout1 = nn.Dropout(0.5)\n","\n","        self.fc2 = nn.Linear(128, 64)\n","        self.BN8 = nn.BatchNorm1d(64)\n","        self.Dropout2 = nn.Dropout(0.5)\n","\n","        self.fc3 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.BN1(x)\n","        x = F.gelu(x)\n","        x = self.conv2(x)\n","        x = self.BN2(x)\n","        x = F.gelu(x)\n","        x = self.pool1(x)\n","\n","        x = self.conv3(x)\n","        x = self.BN3(x)\n","        x = F.gelu(x)\n","        x = self.conv4(x)\n","        x = self.BN4(x)\n","        x = F.gelu(x)\n","        x = self.pool2(x)\n","\n","        x = self.conv5(x)\n","        x = self.BN5(x)\n","        x = F.gelu(x)\n","        x = self.conv6(x)\n","        x = self.BN6(x)\n","        x = F.gelu(x)\n","        x = self.pool3(x)\n","\n","        x = self.flatten(x) # istruzione prof\n","\n","        x = self.fc1(x)\n","        x = self.BN7(x)\n","        x = F.gelu(x)\n","        x = self.Dropout1(x)\n","\n","        x = self.fc2(x)\n","        x = self.BN8(x)\n","        x = F.gelu(x)\n","        x = self.Dropout2(x)\n","\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["model = CNNGodzilla()\n","learning_rate = 0.03\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n","    if torch.backends.mps.is_available() else 'cpu')\n","model = model.to(DEVICE)\n","print(\"Working on\", DEVICE)\n","\n","train_loss_list = []\n","validation_loss_list = []\n","n_epochs = 8 # with the introductions of the Dropout who avoid overfitting we can add some epochs\n","\n","for epoch in range(n_epochs):\n","    loss_train = 0\n","    for data, target in trainloader:\n","        # Set the model in training mode\n","        model.train()\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        # Set the gradient to 0\n","        optimizer.zero_grad()\n","        # Make a prediction\n","        output = model(data)\n","        # Compute the loss function\n","        loss = loss_fn(output, target)\n","        loss_train += loss.item()\n","        # Backpropagation\n","        loss.backward()\n","        # Update parameters\n","        optimizer.step()\n","        \n","    loss_train = loss_train / len(trainloader) # Consider this alternative method of tracking training loss. \n","    train_loss_list.append(loss_train)\n","    \n","    # At the end of every epoch, check the validation loss value\n","    with torch.no_grad():\n","        model.eval()\n","        for data, target in validloader: # Just one batch\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            # Make a prediction\n","            output = model(data)\n","            # Compute the loss function\n","            validation_loss = loss_fn(output, target).item()\n","    print(f\"Epoch {epoch + 1}: Train loss: {loss_train}, Validation loss {validation_loss}\")\n","    validation_loss_list.append(validation_loss)\n","\n","with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    for data, target in testloader:\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        outputs = model(data)\n","        _, predicted = torch.max(outputs.data, 1)\n","        n_samples += target.size(0)\n","        n_correct += (predicted == target).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","print(\"Accuracy on the test set:\", acc, \"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["plt.figure()\n","plt.plot(range(n_epochs), train_loss_list)\n","plt.plot(range(n_epochs), validation_loss_list)\n","plt.legend([\"Train loss\", \"Validation Loss\"])\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss value\")\n","plt.savefig(f\"lossGodzilla\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''\n","Q10 -  Code\n","'''\n","for seed in range(5,10):\n","    torch.manual_seed(seed)\n","    print(\"Seed equal to \", torch.random.initial_seed())\n","    # Train the models here\n","        \n","    model = CNNB()\n","    learning_rate = 0.0302\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n","    loss_fn = nn.CrossEntropyLoss()\n","\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n","        if torch.backends.mps.is_available() else 'cpu')\n","    model = model.to(DEVICE)\n","    print(\"Working on\", DEVICE)\n","\n","    train_loss_list = []\n","    validation_loss_list = []\n","    n_epochs = 4 # with the introductions of the Dropout who avoid overfitting we can add some epochs\n","\n","    for epoch in range(n_epochs):\n","        loss_train = 0\n","        for data, target in trainloader:\n","            # Set the model in training mode\n","            model.train()\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            # Set the gradient to 0\n","            optimizer.zero_grad()\n","            # Make a prediction\n","            output = model(data)\n","            # Compute the loss function\n","            loss = loss_fn(output, target)\n","            loss_train += loss.item()\n","            # Backpropagation\n","            loss.backward()\n","            # Update parameters\n","            optimizer.step()\n","\n","        loss_train = loss_train / len(trainloader) # Consider this alternative method of tracking training loss. \n","        train_loss_list.append(loss_train)\n","    \n","        # At the end of every epoch, check the validation loss value\n","        with torch.no_grad():\n","            model.eval()\n","            for data, target in validloader: # Just one batch\n","                data, target = data.to(DEVICE), target.to(DEVICE)\n","                # Make a prediction\n","                output = model(data)\n","                # Compute the loss function\n","                validation_loss = loss_fn(output, target).item()\n","        print(f\"Epoch {epoch + 1}: Train loss: {loss_train}, Validation loss {validation_loss}\")\n","        validation_loss_list.append(validation_loss)\n","\n","    with torch.no_grad():\n","        n_correct = 0\n","        n_samples = 0\n","        for data, target in testloader:\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            outputs = model(data)\n","            _, predicted = torch.max(outputs.data, 1)\n","            n_samples += target.size(0)\n","            n_correct += (predicted == target).sum().item()\n","\n","        acc = 100.0 * n_correct / n_samples\n","    print(\"Accuracy on the test set:\", acc, \"%\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"deeple","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"}},"nbformat":4,"nbformat_minor":4}
