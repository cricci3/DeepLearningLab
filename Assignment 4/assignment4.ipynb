{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CLAUDIO RICCI"]},{"cell_type":"markdown","metadata":{},"source":["## Packages"]},{"cell_type":"code","execution_count":326,"metadata":{"ExecuteTime":{"end_time":"2024-12-05T09:31:30.777483Z","start_time":"2024-12-05T09:31:30.704856Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-12-27T16:13:17.043091Z","iopub.status.busy":"2024-12-27T16:13:17.042696Z","iopub.status.idle":"2024-12-27T16:13:17.048183Z","shell.execute_reply":"2024-12-27T16:13:17.047339Z","shell.execute_reply.started":"2024-12-27T16:13:17.043061Z"},"trusted":true},"outputs":[],"source":["import networkx as nx # For graphs\n","import pickle # For data parsing\n","from networkx.algorithms.approximation import greedy_tsp # For approx TSP\n","import numpy as np\n","\n","import torch\n","from torch.utils.data import DataLoader, random_split, Dataset\n","import torch.nn as nn\n","from torch.nn import Transformer\n","from torch import Tensor\n","\n","import warnings\n","from timeit import default_timer as timer\n","import math"]},{"cell_type":"markdown","metadata":{},"source":["## Helper functions"]},{"cell_type":"code","execution_count":327,"metadata":{"ExecuteTime":{"end_time":"2024-12-05T09:31:30.806883Z","start_time":"2024-12-05T09:31:30.798896Z"},"execution":{"iopub.execute_input":"2024-12-27T16:13:17.056752Z","iopub.status.busy":"2024-12-27T16:13:17.056497Z","iopub.status.idle":"2024-12-27T16:13:17.070000Z","shell.execute_reply":"2024-12-27T16:13:17.069087Z","shell.execute_reply.started":"2024-12-27T16:13:17.056727Z"},"trusted":true},"outputs":[],"source":["def tour_length(G, tour):\n","    \"\"\"\n","    Compute the length of a tour. A tour is a list having elments 0 and -1 equal\n","    \"\"\"\n","    assert tour[0] == tour[-1], \"Not valid tour\"\n","    estimated = 0\n","    for i in range(n):\n","        estimated += G[tour[i]][tour[i + 1]]['weight']\n","    return estimated\n","\n","def greedy_algorithm(G):\n","    \"\"\"\n","    Run the value of the greedy approximation algorithm on graph G\n","    \"\"\"\n","    return tour_length(G, greedy_tsp(G, weight='weight'))\n","\n","def random_tour(G, seed = 42):\n","    \"\"\"\n","    Return the value of a random tour\n","    \"\"\"\n","    np.random.seed(seed)\n","    n = G.number_of_nodes()\n","    tour = [0]\n","    for i in range(1, n):\n","        next_node = np.random.choice([j for j in range(n) if j not in tour])\n","        tour.append(next_node)\n","    tour.append(0)\n","\n","def transformer_tsp(G, model, DEVICE = 'cpu'):\n","    \"\"\"\n","    Evaluate your (trained) model on G\n","    \"\"\"\n","    # Set the model in evaluation mode\n","    model.eval()\n","\n","    # Note: number of edges is constant ed equal to n(n-1)/2\n","    n = G.number_of_nodes()\n","    \n","    # Get node coordinates\n","    attr = nx.get_node_attributes(G, 'pos')\n","    x = []\n","    for i in range(n):\n","        x.append(torch.tensor(attr[i], dtype=torch.float32))\n","\n","    # From list of tensors to tensor 2d\n","    x = torch.stack(x)    \n","\n","    tour = [0]\n","    y = torch.tensor(tour, dtype=torch.long)\n","    x = x.to(DEVICE).unsqueeze(0)\n","    y = y.to(DEVICE).unsqueeze(0)\n","    \n","    # Predict the next node\n","    out = transformer_model(x, y)\n","    \n","    # Loop until the tour is complete\n","    while len(tour) < n:\n","        _, idx = torch.topk(out, n, dim=2)\n","        for i in range(n):\n","            # Check if the node is already in the tour\n","            if idx[0, 0, i] not in tour:\n","                tour.append(idx[0, 0, i])\n","                break\n","        y = torch.tensor(tour)\n","        y = y.to(DEVICE).unsqueeze(0)\n","        out = transformer_model(x, y)\n","    \n","    tour = [int(i) for i in tour] + [0] # Append the starting node (that is hard-coded to 0)\n","    return tour_length(G, tour)\n","\n","\n","\n","def gap(G, model = None, model_GA = None, random_seed = 42, device = 'cpu'):\n","    \"\"\"\n","    Compute the gap between the optimal solution on graph G and all the analyzed methods\n","    \"\"\"\n","\n","        \n","    # Optimal value (hard-coded in the graph)\n","    TSP = sum([G[i][j]['weight']*G[i][j]['tour'] for (i, j) in G.edges()]) # Optimal\n","\n","    # Gaps dictionary\n","    gaps = {'greedy' : 0, 'random' : 0, 'transformer_tsp': 0, 'transformer_tsp_acc_grad': 0}\n","    gaps['greedy'] = 100* (greedy_algorithm(G) -  TSP) / TSP\n","    gaps['random'] = 100 * (random_tour(G, random_seed) - TSP) / TSP\n","    if model is not None:\n","        gaps['transformer_tsp'] = 100 * (transformer_tsp(G, model, DEVICE=device) - TSP) / TSP\n","    else:\n","        gaps['transformer_tsp'] = float('inf') # In case you just train with GA\n","        \n","    if model_GA is not None:\n","        gaps['transformer_tsp_acc_grad'] = 100 * (transformer_tsp(G, model_GA, DEVICE=device) - TSP) / TSP\n","    else:\n","        gaps['transformer_tsp_acc_grad'] = float('inf') # In case you just train without GA\n","    return gaps    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Dataset & Dataloader"]},{"cell_type":"code","execution_count":328,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.071420Z","iopub.status.busy":"2024-12-27T16:13:17.071178Z","iopub.status.idle":"2024-12-27T16:13:17.104613Z","shell.execute_reply":"2024-12-27T16:13:17.103747Z","shell.execute_reply.started":"2024-12-27T16:13:17.071396Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'list'>\n","Type of a element of the dataset:  <class 'tuple'>\n","Type of the first item of the tuple:  <class 'networkx.classes.graph.Graph'>\n","Type of the second item of the tuple:  <class 'list'>\n","(<networkx.classes.graph.Graph object at 0x7850b9049240>, [0, 3, 14, 2, 9, 6, 19, 13, 12, 16, 7, 18, 8, 17, 5, 11, 10, 15, 1, 4, 0])\n"]}],"source":["# Load the dummy dataset, get a single data item and explain its Python type\n","with open('/kaggle/input/tspinstances/dummy_20_DLL_ass4.pkl', 'rb') as file:\n","    dummy = pickle.load(file)\n","\n","print(type(dummy)) # list\n","print('Type of a element of the dataset: ', type(dummy[0]))  # The type of the first object -> tuple\n","print('Type of the first item of the tuple: ', type(dummy[0][0]))  # The type of the first item of a tuple -> Graph\n","print('Type of the second item of the tuple: ', type(dummy[0][1]))  # The type of the first item of a tuple -> list\n","print(dummy[0])"]},{"cell_type":"code","execution_count":329,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.106270Z","iopub.status.busy":"2024-12-27T16:13:17.106001Z","iopub.status.idle":"2024-12-27T16:13:17.111920Z","shell.execute_reply":"2024-12-27T16:13:17.111035Z","shell.execute_reply.started":"2024-12-27T16:13:17.106246Z"},"trusted":true},"outputs":[{"data":{"text/plain":["EdgeView([(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 18), (3, 19), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (4, 16), (4, 17), (4, 18), (4, 19), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 18), (5, 19), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (6, 16), (6, 17), (6, 18), (6, 19), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 18), (7, 19), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (8, 16), (8, 17), (8, 18), (8, 19), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (11, 19), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (13, 19), (14, 15), (14, 16), (14, 17), (14, 18), (14, 19), (15, 16), (15, 17), (15, 18), (15, 19), (16, 17), (16, 18), (16, 19), (17, 18), (17, 19), (18, 19)])"]},"execution_count":329,"metadata":{},"output_type":"execute_result"}],"source":["dummy[0][0].edges"]},{"cell_type":"code","execution_count":330,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.113277Z","iopub.status.busy":"2024-12-27T16:13:17.112949Z","iopub.status.idle":"2024-12-27T16:13:17.121172Z","shell.execute_reply":"2024-12-27T16:13:17.120350Z","shell.execute_reply.started":"2024-12-27T16:13:17.113236Z"},"trusted":true},"outputs":[],"source":["# Describe the edge attributes tour and weight\n","\n","# Extract the graph and tour\n","graph = dummy[0][0]  # The networkx Graph object\n","tour = dummy[0][1]   # The tour as a list of nodes\n","\n","# Inspect edges with attributes\n","# print(\"Edges and attributes:\")\n","# for u, v, data in graph.edges(data=True):\n","#     print(f\"  Edge ({u}, {v}):\")\n","#     print('     Weight: ', {data.get('weight', 'Not found')})\n","#     print('     Tour: ',{data.get('tour', 'Not found')})"]},{"cell_type":"code","execution_count":331,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.123750Z","iopub.status.busy":"2024-12-27T16:13:17.123182Z","iopub.status.idle":"2024-12-27T16:13:17.135133Z","shell.execute_reply":"2024-12-27T16:13:17.134202Z","shell.execute_reply.started":"2024-12-27T16:13:17.123724Z"},"trusted":true},"outputs":[],"source":["# Inspect the node attribute pos\n","# print(\"\\nNodes and attributes:\")\n","# for node, data in graph.nodes(data=True):\n","#     print(f\"  Node {node}:\")\n","#     print('     Position: ',{data.get('pos', 'Not found')})"]},{"cell_type":"code","execution_count":332,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.136267Z","iopub.status.busy":"2024-12-27T16:13:17.136043Z","iopub.status.idle":"2024-12-27T16:13:17.144358Z","shell.execute_reply":"2024-12-27T16:13:17.143544Z","shell.execute_reply.started":"2024-12-27T16:13:17.136243Z"},"trusted":true},"outputs":[],"source":["# # Analyze the tour\n","# print(\"\\nTour edges:\")\n","# tour_edges = [(tour[i], tour[i+1]) for i in range(len(tour) - 1)]\n","# for u, v in tour_edges:\n","#     if graph.has_edge(u, v):\n","#         print(f\"   Edge ({u}, {v}) exists with weight {graph[u][v].get('weight', 'Not found')}.\")\n","#     else:\n","#         print(f\"   Edge ({u}, {v}) does not exist in the graph.\")"]},{"cell_type":"code","execution_count":333,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.145469Z","iopub.status.busy":"2024-12-27T16:13:17.145235Z","iopub.status.idle":"2024-12-27T16:13:17.155430Z","shell.execute_reply":"2024-12-27T16:13:17.154749Z","shell.execute_reply.started":"2024-12-27T16:13:17.145445Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(0)\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":334,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.176029Z","iopub.status.busy":"2024-12-27T16:13:17.175556Z","iopub.status.idle":"2024-12-27T16:13:17.181760Z","shell.execute_reply":"2024-12-27T16:13:17.180840Z","shell.execute_reply.started":"2024-12-27T16:13:17.176004Z"},"trusted":true},"outputs":[],"source":["# Implement a dataset class. Focus on the getitem method to return:\n","# – X: A tensor of node coordinates with size 20 × 2.\n","# – y: A tour starting from 0 and ending with 0.\n","\n","class GraphDataset(Dataset):\n","    def __init__(self, data):\n","        \"\"\"\n","        A list of tuples where each tuple contains:\n","        - A networkx.Graph object\n","        - A tour (list of node indices)\n","        \"\"\"\n","        self.data = data\n","\n","    def __len__(self):\n","        \"\"\"\n","        Number of instances in the dataset\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns:\n","        - X: A tensor of node coordinates with size 20 × 2.\n","        - y: A tensor representing the tour, starting and ending at 0.\n","        \"\"\"\n","        # Extract the graph and tour\n","        graph, tour = self.data[idx]\n","\n","        # Get node positions as a 2D array\n","        pos = nx.get_node_attributes(graph, 'pos')  # Dictionary {node: (x, y)}\n","        if not pos:\n","            raise ValueError(f\"Graph at index {idx} is missing node positions ('pos').\")\n","\n","        # Ensure nodes are sorted by their index (important for consistent tensor order)\n","        sorted_positions = [pos[node] for node in sorted(graph.nodes())]\n","        \n","        # Convert positions to a tensor of shape (20, 2)\n","        X = torch.tensor(sorted_positions, dtype=torch.float32)\n","\n","        # Convert the tour to a tensor\n","        y = torch.tensor(tour, dtype=torch.long)\n","\n","        return X, y"]},{"cell_type":"code","execution_count":335,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.190661Z","iopub.status.busy":"2024-12-27T16:13:17.190251Z","iopub.status.idle":"2024-12-27T16:13:17.195543Z","shell.execute_reply":"2024-12-27T16:13:17.194668Z","shell.execute_reply.started":"2024-12-27T16:13:17.190635Z"},"trusted":true},"outputs":[],"source":["# Create Dataset objects for training, validation, and testing, along with their respective Dataloader\n","dataset = GraphDataset(dummy)\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])\n","\n","batch_size = 32\n","\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":336,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.197133Z","iopub.status.busy":"2024-12-27T16:13:17.196889Z","iopub.status.idle":"2024-12-27T16:13:17.208105Z","shell.execute_reply":"2024-12-27T16:13:17.206842Z","shell.execute_reply.started":"2024-12-27T16:13:17.197110Z"},"trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self,emb_size: int, dropout: float, maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)   \n","        \n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","class TSPTransformer(nn.Module):\n","    def __init__(self, n, num_encoder, num_decoder, de, dd, n_head, dropout):\n","        super(TSPTransformer, self).__init__()\n","        # Encoder\n","        self.linear1 = nn.Linear(2, de)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=de, nhead=n_head) # d_model (int) – the number of expected features in the input (required).\n","        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder) # stack of n encoder layers\n","        self.linear2 = nn.Linear(de, dd)\n","        \n","        # Decoder\n","        self.embedding = nn.Embedding(n, dd)\n","        self.posEncoding = PositionalEncoding(dd, dropout)\n","        decoder_layer = nn.TransformerDecoderLayer(d_model=dd, nhead=n_head)\n","        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder)\n","        self.ffnn = nn.Linear(dd, n)\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor):\n","        \n","        # Encoding\n","        src = self.linear1(src)\n","        # Change: No mask for the encoder or an additive mask of zeros\n","        src = self.encoder(src, src_key_padding_mask=src_padding_mask)\n","        src = self.linear2(src)\n","\n","        # Decoding\n","        trg = self.embedding(trg)\n","        trg = self.posEncoding(trg)\n","        output = self.decoder(trg, src, tgt_mask=tgt_mask, memory_mask=None,\n","                            tgt_key_padding_mask=tgt_padding_mask,\n","                            memory_key_padding_mask=src_padding_mask)\n","        output = self.ffnn(output)\n","        return output"]},{"cell_type":"code","execution_count":337,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.209358Z","iopub.status.busy":"2024-12-27T16:13:17.209121Z","iopub.status.idle":"2024-12-27T16:13:17.223369Z","shell.execute_reply":"2024-12-27T16:13:17.222629Z","shell.execute_reply.started":"2024-12-27T16:13:17.209333Z"},"trusted":true},"outputs":[],"source":["def generate_square_subsequent_mask(sz, DEVICE):\n","    ## Decoder mask\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","\n","def create_mask(src, tgt, DEVICE, num_heads):\n","    src_seq_len = src.shape[1]\n","    tgt_seq_len = tgt.shape[1]\n","    batch_size = src.shape[0]\n","\n","    # Source mask\n","    #src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).bool()\n","    #src_mask = src_mask.unsqueeze(0).expand(batch_size, -1, -1)\n","\n","    # Target mask\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, DEVICE)\n","    tgt_mask = tgt_mask.unsqueeze(0).expand(batch_size, -1, -1)\n","\n","    # Combined attention mask\n","    # attn_mask = torch.zeros((batch_size * num_heads, tgt_seq_len, src_seq_len), device=DEVICE).bool()\n","    # for i in range(batch_size):\n","    #     attn_mask[i*num_heads:(i+1)*num_heads, :, :] = torch.logical_or(src_mask[i], tgt_mask[i].unsqueeze(0))\n","\n","    # Padding masks (Corrected dimensions and type)\n","    src_padding_mask = torch.zeros((batch_size, src_seq_len), device=DEVICE).bool()\n","    tgt_padding_mask = torch.zeros((batch_size, tgt_seq_len), device=DEVICE).bool()\n","\n","    src_padding_mask = src_padding_mask.permute(1, 0)  # Reshape to (batch_size, source_sequence_length)\n","\n","    return tgt_mask, src_padding_mask, tgt_padding_mask"]},{"cell_type":"markdown","metadata":{},"source":["$n$ = num di nodes -> Each TSP instance has exactly 20 nodes, and the input consists of their 2D coordinates.\n","\n","$de$ = size of the internal rapresentation of the input coordinates\n","       2 feature per node -> small values like 16, 32, 64\n","\n","$dd$ = size of intermediate representation\n","     y is a sequence of discrete node indeces, typical equal to de but also higher\n","\n","$Ne$ = num encoeer layers -> 2,4,6\n","\n","$Nd$ = num decoder layers -> equal to Ne but also higher"]},{"cell_type":"code","execution_count":338,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.224751Z","iopub.status.busy":"2024-12-27T16:13:17.224409Z","iopub.status.idle":"2024-12-27T16:13:17.257350Z","shell.execute_reply":"2024-12-27T16:13:17.256538Z","shell.execute_reply.started":"2024-12-27T16:13:17.224709Z"},"trusted":true},"outputs":[],"source":["n = 20\n","n_enc = 4\n","n_dec = 4\n","de = 32\n","dd = 64\n","N_HEAD = 8\n","DROPOUT = 0.1\n","\n","TSPmodel = TSPTransformer(n, n_enc, n_dec, de, dd, N_HEAD, DROPOUT).to(DEVICE)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":339,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.259110Z","iopub.status.busy":"2024-12-27T16:13:17.258861Z","iopub.status.idle":"2024-12-27T16:13:17.265542Z","shell.execute_reply":"2024-12-27T16:13:17.264620Z","shell.execute_reply.started":"2024-12-27T16:13:17.259084Z"},"trusted":true},"outputs":[],"source":["# Function for training a single epoch\n","def train_epoch(model, optimizer, trainloader, loss_fn, DEVICE, num_heads):\n","    model.train()\n","    losses = 0\n","\n","    for src, tgt in trainloader:\n","        src = src.to(DEVICE)  # Node coordinates (input to the encoder)\n","        tgt = tgt.to(DEVICE)  # Tour indices (target sequence for the decoder)\n","        print(src.shape, tgt.shape)\n","\n","        tgt_input = tgt[:, :-1]  # Input to the decoder (shifted by one token)\n","        tgt_out = tgt[:, 1:]  # Target for loss computation (shifted by one token)\n","\n","        optimizer.zero_grad()\n","\n","        # Generate masks for attention\n","        tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, DEVICE, num_heads)\n","\n","        # Forward pass\n","        output = model(src, tgt_input, tgt_mask, src_padding_mask, tgt_padding_mask)\n","\n","        loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        # Update weights\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(trainloader)"]},{"cell_type":"code","execution_count":340,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.266925Z","iopub.status.busy":"2024-12-27T16:13:17.266601Z","iopub.status.idle":"2024-12-27T16:13:17.280351Z","shell.execute_reply":"2024-12-27T16:13:17.279609Z","shell.execute_reply.started":"2024-12-27T16:13:17.266890Z"},"trusted":true},"outputs":[],"source":["# Function for evaluation\n","def evaluate(model, valloader, loss_fn, DEVICE, num_heads):\n","    model.eval()\n","    losses = 0\n","\n","    with torch.no_grad():\n","        for src, tgt in valloader:\n","            src = src.to(DEVICE)  # Node coordinates (input to the encoder)\n","            tgt = tgt.to(DEVICE)  # Tour indices (target sequence for the decoder)\n","          \n","            tgt_input = tgt[:, :-1]  # Input to the decoder (shifted by one token)\n","            tgt_out = tgt[:, 1:]  # Target for loss computation (shifted by one token)\n","\n","            # Generate masks for attention\n","            tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, DEVICE, num_heads)\n","\n","            # Forward pass\n","            output = model(src, tgt_input, tgt_mask, src_padding_mask, tgt_padding_mask)\n","\n","            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n","            losses += loss.item()\n","\n","    avg_loss = losses / len(valloader)\n","    return avg_loss"]},{"cell_type":"code","execution_count":341,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T16:13:17.281638Z","iopub.status.busy":"2024-12-27T16:13:17.281358Z","iopub.status.idle":"2024-12-27T16:13:17.764299Z","shell.execute_reply":"2024-12-27T16:13:17.762908Z","shell.execute_reply.started":"2024-12-27T16:13:17.281611Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 20, 2]) torch.Size([32, 21])\n"]},{"ename":"RuntimeError","evalue":"The shape of the 3D attn_mask is torch.Size([32, 20, 20]), but should be (160, 32, 32).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[341], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTSPmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_HEAD\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate on validation data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate(TSPmodel, valloader, loss_fn, DEVICE, N_HEAD)\n","Cell \u001b[0;32mIn[339], line 20\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, trainloader, loss_fn, DEVICE, num_heads)\u001b[0m\n\u001b[1;32m     17\u001b[0m tgt_mask, src_padding_mask, tgt_padding_mask \u001b[38;5;241m=\u001b[39m create_mask(src, tgt_input, DEVICE, num_heads)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), tgt_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[336], line 51\u001b[0m, in \u001b[0;36mTSPTransformer.forward\u001b[0;34m(self, src, trg, tgt_mask, src_padding_mask, tgt_padding_mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m trg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(trg)\n\u001b[1;32m     50\u001b[0m trg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposEncoding(trg)\n\u001b[0;32m---> 51\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffnn(output)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:495\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    492\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 495\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:890\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    888\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 890\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    891\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    892\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:899\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    898\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 899\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5443\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5441\u001b[0m     correct_3d_size \u001b[38;5;241m=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m num_heads, tgt_len, src_len)\n\u001b[1;32m   5442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m correct_3d_size:\n\u001b[0;32m-> 5443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the 3D attn_mask is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_3d_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: The shape of the 3D attn_mask is torch.Size([32, 20, 20]), but should be (160, 32, 32)."]}],"source":["warnings.filterwarnings(\"ignore\")\n","\n","# hyperparameters\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(TSPmodel.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)\n","\n","# Training configuration\n","NUM_EPOCHS = 10\n","training_losses = []\n","val_losses = []\n","\n","# Training loop\n","for epoch in range(1, NUM_EPOCHS + 1):\n","    start_time = timer()\n","    \n","    # Train for one epoch\n","    train_loss = train_epoch(TSPmodel, optimizer, trainloader, loss_fn, DEVICE, N_HEAD)    \n","    \n","    # Evaluate on validation data\n","    val_loss = evaluate(TSPmodel, valloader, loss_fn, DEVICE, N_HEAD)\n","    end_time = timer()\n","\n","    # Log losses\n","    training_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    \n","    print(f\"Epoch: {epoch}\")\n","    print(f\"Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Perplexity: {perplexity:.3f}\")\n","    print(f\"Epoch time = {(end_time - start_time):.3f}s\")"]},{"cell_type":"markdown","metadata":{},"source":["### Training WITHOUT gradient accumulation"]},{"cell_type":"markdown","metadata":{},"source":["### Training WITH gradient accumulation"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6323224,"sourceId":10227445,"sourceType":"datasetVersion"}],"dockerImageVersionId":30804,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
