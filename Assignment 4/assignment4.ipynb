{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CLAUDIO RICCI"]},{"cell_type":"markdown","metadata":{},"source":["## Packages"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2024-12-05T09:31:30.777483Z","start_time":"2024-12-05T09:31:30.704856Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-12-27T15:02:13.887568Z","iopub.status.busy":"2024-12-27T15:02:13.886875Z","iopub.status.idle":"2024-12-27T15:02:13.892243Z","shell.execute_reply":"2024-12-27T15:02:13.891281Z","shell.execute_reply.started":"2024-12-27T15:02:13.887538Z"},"trusted":true},"outputs":[],"source":["import networkx as nx # For graphs\n","import pickle # For data parsing\n","from networkx.algorithms.approximation import greedy_tsp # For approx TSP\n","import numpy as np\n","\n","import torch\n","from torch.utils.data import DataLoader, random_split, Dataset\n","import torch.nn as nn\n","from torch.nn import Transformer\n","from torch import Tensor\n","\n","import warnings\n","from timeit import default_timer as timer\n","import math"]},{"cell_type":"markdown","metadata":{},"source":["## Helper functions"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2024-12-05T09:31:30.806883Z","start_time":"2024-12-05T09:31:30.798896Z"},"execution":{"iopub.execute_input":"2024-12-27T15:02:14.097489Z","iopub.status.busy":"2024-12-27T15:02:14.097155Z","iopub.status.idle":"2024-12-27T15:02:14.111406Z","shell.execute_reply":"2024-12-27T15:02:14.110572Z","shell.execute_reply.started":"2024-12-27T15:02:14.097461Z"},"trusted":true},"outputs":[],"source":["def tour_length(G, tour):\n","    \"\"\"\n","    Compute the length of a tour. A tour is a list having elments 0 and -1 equal\n","    \"\"\"\n","    assert tour[0] == tour[-1], \"Not valid tour\"\n","    estimated = 0\n","    for i in range(n):\n","        estimated += G[tour[i]][tour[i + 1]]['weight']\n","    return estimated\n","\n","def greedy_algorithm(G):\n","    \"\"\"\n","    Run the value of the greedy approximation algorithm on graph G\n","    \"\"\"\n","    return tour_length(G, greedy_tsp(G, weight='weight'))\n","\n","def random_tour(G, seed = 42):\n","    \"\"\"\n","    Return the value of a random tour\n","    \"\"\"\n","    np.random.seed(seed)\n","    n = G.number_of_nodes()\n","    tour = [0]\n","    for i in range(1, n):\n","        next_node = np.random.choice([j for j in range(n) if j not in tour])\n","        tour.append(next_node)\n","    tour.append(0)\n","\n","def transformer_tsp(G, model, DEVICE = 'cpu'):\n","    \"\"\"\n","    Evaluate your (trained) model on G\n","    \"\"\"\n","    # Set the model in evaluation mode\n","    model.eval()\n","\n","    # Note: number of edges is constant ed equal to n(n-1)/2\n","    n = G.number_of_nodes()\n","    \n","    # Get node coordinates\n","    attr = nx.get_node_attributes(G, 'pos')\n","    x = []\n","    for i in range(n):\n","        x.append(torch.tensor(attr[i], dtype=torch.float32))\n","\n","    # From list of tensors to tensor 2d\n","    x = torch.stack(x)    \n","\n","    tour = [0]\n","    y = torch.tensor(tour, dtype=torch.long)\n","    x = x.to(DEVICE).unsqueeze(0)\n","    y = y.to(DEVICE).unsqueeze(0)\n","    \n","    # Predict the next node\n","    out = transformer_model(x, y)\n","    \n","    # Loop until the tour is complete\n","    while len(tour) < n:\n","        _, idx = torch.topk(out, n, dim=2)\n","        for i in range(n):\n","            # Check if the node is already in the tour\n","            if idx[0, 0, i] not in tour:\n","                tour.append(idx[0, 0, i])\n","                break\n","        y = torch.tensor(tour)\n","        y = y.to(DEVICE).unsqueeze(0)\n","        out = transformer_model(x, y)\n","    \n","    tour = [int(i) for i in tour] + [0] # Append the starting node (that is hard-coded to 0)\n","    return tour_length(G, tour)\n","\n","\n","\n","def gap(G, model = None, model_GA = None, random_seed = 42, device = 'cpu'):\n","    \"\"\"\n","    Compute the gap between the optimal solution on graph G and all the analyzed methods\n","    \"\"\"\n","\n","        \n","    # Optimal value (hard-coded in the graph)\n","    TSP = sum([G[i][j]['weight']*G[i][j]['tour'] for (i, j) in G.edges()]) # Optimal\n","\n","    # Gaps dictionary\n","    gaps = {'greedy' : 0, 'random' : 0, 'transformer_tsp': 0, 'transformer_tsp_acc_grad': 0}\n","    gaps['greedy'] = 100* (greedy_algorithm(G) -  TSP) / TSP\n","    gaps['random'] = 100 * (random_tour(G, random_seed) - TSP) / TSP\n","    if model is not None:\n","        gaps['transformer_tsp'] = 100 * (transformer_tsp(G, model, DEVICE=device) - TSP) / TSP\n","    else:\n","        gaps['transformer_tsp'] = float('inf') # In case you just train with GA\n","        \n","    if model_GA is not None:\n","        gaps['transformer_tsp_acc_grad'] = 100 * (transformer_tsp(G, model_GA, DEVICE=device) - TSP) / TSP\n","    else:\n","        gaps['transformer_tsp_acc_grad'] = float('inf') # In case you just train without GA\n","    return gaps    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Dataset & Dataloader"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.112910Z","iopub.status.busy":"2024-12-27T15:02:14.112606Z","iopub.status.idle":"2024-12-27T15:02:14.255329Z","shell.execute_reply":"2024-12-27T15:02:14.254528Z","shell.execute_reply.started":"2024-12-27T15:02:14.112885Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'list'>\n","Type of a element of the dataset:  <class 'tuple'>\n","Type of the first item of the tuple:  <class 'networkx.classes.graph.Graph'>\n","Type of the second item of the tuple:  <class 'list'>\n","(<networkx.classes.graph.Graph object at 0x7851001e5420>, [0, 3, 14, 2, 9, 6, 19, 13, 12, 16, 7, 18, 8, 17, 5, 11, 10, 15, 1, 4, 0])\n"]}],"source":["# Load the dummy dataset, get a single data item and explain its Python type\n","with open('/kaggle/input/tspinstances/dummy_20_DLL_ass4.pkl', 'rb') as file:\n","    dummy = pickle.load(file)\n","\n","print(type(dummy)) # list\n","print('Type of a element of the dataset: ', type(dummy[0]))  # The type of the first object -> tuple\n","print('Type of the first item of the tuple: ', type(dummy[0][0]))  # The type of the first item of a tuple -> Graph\n","print('Type of the second item of the tuple: ', type(dummy[0][1]))  # The type of the first item of a tuple -> list\n","print(dummy[0])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.256672Z","iopub.status.busy":"2024-12-27T15:02:14.256411Z","iopub.status.idle":"2024-12-27T15:02:14.262886Z","shell.execute_reply":"2024-12-27T15:02:14.262069Z","shell.execute_reply.started":"2024-12-27T15:02:14.256646Z"},"trusted":true},"outputs":[{"data":{"text/plain":["EdgeView([(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 18), (3, 19), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (4, 16), (4, 17), (4, 18), (4, 19), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 18), (5, 19), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (6, 16), (6, 17), (6, 18), (6, 19), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 18), (7, 19), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (8, 16), (8, 17), (8, 18), (8, 19), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (11, 19), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (13, 19), (14, 15), (14, 16), (14, 17), (14, 18), (14, 19), (15, 16), (15, 17), (15, 18), (15, 19), (16, 17), (16, 18), (16, 19), (17, 18), (17, 19), (18, 19)])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dummy[0][0].edges"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.265624Z","iopub.status.busy":"2024-12-27T15:02:14.265007Z","iopub.status.idle":"2024-12-27T15:02:14.281435Z","shell.execute_reply":"2024-12-27T15:02:14.280668Z","shell.execute_reply.started":"2024-12-27T15:02:14.265586Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Edges and attributes:\n","  Edge (0, 1):\n","     Weight:  {0.4287846201876535}\n","     Tour:  {0}\n","  Edge (0, 2):\n","     Weight:  {0.20417626260418414}\n","     Tour:  {0}\n","  Edge (0, 3):\n","     Weight:  {0.08154537102129383}\n","     Tour:  {1}\n","  Edge (0, 4):\n","     Weight:  {0.08031174728403137}\n","     Tour:  {1}\n","  Edge (0, 5):\n","     Weight:  {0.5612080164024046}\n","     Tour:  {0}\n","  Edge (0, 6):\n","     Weight:  {0.2295012043541082}\n","     Tour:  {0}\n","  Edge (0, 7):\n","     Weight:  {0.514746498076055}\n","     Tour:  {0}\n","  Edge (0, 8):\n","     Weight:  {0.5266704818162102}\n","     Tour:  {0}\n","  Edge (0, 9):\n","     Weight:  {0.15283720045858457}\n","     Tour:  {0}\n","  Edge (0, 10):\n","     Weight:  {0.3556451198878696}\n","     Tour:  {0}\n","  Edge (0, 11):\n","     Weight:  {0.24046942531001395}\n","     Tour:  {0}\n","  Edge (0, 12):\n","     Weight:  {0.3189080728571583}\n","     Tour:  {0}\n","  Edge (0, 13):\n","     Weight:  {0.49621109228471955}\n","     Tour:  {0}\n","  Edge (0, 14):\n","     Weight:  {0.1765729477183786}\n","     Tour:  {0}\n","  Edge (0, 15):\n","     Weight:  {0.4701608359103749}\n","     Tour:  {0}\n","  Edge (0, 16):\n","     Weight:  {0.3309416339870591}\n","     Tour:  {0}\n","  Edge (0, 17):\n","     Weight:  {0.5383964713091507}\n","     Tour:  {0}\n","  Edge (0, 18):\n","     Weight:  {0.4857390315454929}\n","     Tour:  {0}\n","  Edge (0, 19):\n","     Weight:  {0.2955332394593224}\n","     Tour:  {0}\n","  Edge (1, 2):\n","     Weight:  {0.6148965694180403}\n","     Tour:  {0}\n","  Edge (1, 3):\n","     Weight:  {0.458726401538421}\n","     Tour:  {0}\n","  Edge (1, 4):\n","     Weight:  {0.35637320892343344}\n","     Tour:  {1}\n","  Edge (1, 5):\n","     Weight:  {0.6032864976885546}\n","     Tour:  {0}\n","  Edge (1, 6):\n","     Weight:  {0.6569915301709606}\n","     Tour:  {0}\n","  Edge (1, 7):\n","     Weight:  {0.9207746398930378}\n","     Tour:  {0}\n","  Edge (1, 8):\n","     Weight:  {0.746742011300412}\n","     Tour:  {0}\n","  Edge (1, 9):\n","     Weight:  {0.5810402333348532}\n","     Tour:  {0}\n","  Edge (1, 10):\n","     Weight:  {0.16704392462309525}\n","     Tour:  {0}\n","  Edge (1, 11):\n","     Weight:  {0.44648016447297073}\n","     Tour:  {0}\n","  Edge (1, 12):\n","     Weight:  {0.7422384454019971}\n","     Tour:  {0}\n","  Edge (1, 13):\n","     Weight:  {0.9087485603102635}\n","     Tour:  {0}\n","  Edge (1, 14):\n","     Weight:  {0.5479091406494869}\n","     Tour:  {0}\n","  Edge (1, 15):\n","     Weight:  {0.14178198392466582}\n","     Tour:  {1}\n","  Edge (1, 16):\n","     Weight:  {0.754354006043937}\n","     Tour:  {0}\n","  Edge (1, 17):\n","     Weight:  {0.6438862448532572}\n","     Tour:  {0}\n","  Edge (1, 18):\n","     Weight:  {0.7726546814458304}\n","     Tour:  {0}\n","  Edge (1, 19):\n","     Weight:  {0.5987646096934333}\n","     Tour:  {0}\n","  Edge (2, 3):\n","     Weight:  {0.15622130264510733}\n","     Tour:  {0}\n","  Edge (2, 4):\n","     Weight:  {0.2841510683911888}\n","     Tour:  {0}\n","  Edge (2, 5):\n","     Weight:  {0.5519190450546716}\n","     Tour:  {0}\n","  Edge (2, 6):\n","     Weight:  {0.13946205995874017}\n","     Tour:  {0}\n","  Edge (2, 7):\n","     Weight:  {0.3114220898670106}\n","     Tour:  {0}\n","  Edge (2, 8):\n","     Weight:  {0.4194690047798088}\n","     Tour:  {0}\n","  Edge (2, 9):\n","     Weight:  {0.08856321635783988}\n","     Tour:  {1}\n","  Edge (2, 10):\n","     Weight:  {0.5083743055689474}\n","     Tour:  {0}\n","  Edge (2, 11):\n","     Weight:  {0.2644009954540205}\n","     Tour:  {0}\n","  Edge (2, 12):\n","     Weight:  {0.2208301469437315}\n","     Tour:  {0}\n","  Edge (2, 13):\n","     Weight:  {0.4003621671541578}\n","     Tour:  {0}\n","  Edge (2, 14):\n","     Weight:  {0.09093880543328159}\n","     Tour:  {1}\n","  Edge (2, 15):\n","     Weight:  {0.6278926750270484}\n","     Tour:  {0}\n","  Edge (2, 16):\n","     Weight:  {0.14517438585231934}\n","     Tour:  {0}\n","  Edge (2, 17):\n","     Weight:  {0.49907142344556654}\n","     Tour:  {0}\n","  Edge (2, 18):\n","     Weight:  {0.34060003464189553}\n","     Tour:  {0}\n","  Edge (2, 19):\n","     Weight:  {0.3852352221449024}\n","     Tour:  {0}\n","  Edge (3, 4):\n","     Weight:  {0.14654472382192782}\n","     Tour:  {0}\n","  Edge (3, 5):\n","     Weight:  {0.4946628652419162}\n","     Tour:  {0}\n","  Edge (3, 6):\n","     Weight:  {0.2312084560472005}\n","     Tour:  {0}\n","  Edge (3, 7):\n","     Weight:  {0.46429696895897604}\n","     Tour:  {0}\n","  Edge (3, 8):\n","     Weight:  {0.4453880485781254}\n","     Tour:  {0}\n","  Edge (3, 9):\n","     Weight:  {0.14324743187115518}\n","     Tour:  {0}\n","  Edge (3, 10):\n","     Weight:  {0.357006300672558}\n","     Tour:  {0}\n","  Edge (3, 11):\n","     Weight:  {0.17313884874554414}\n","     Tour:  {0}\n","  Edge (3, 12):\n","     Weight:  {0.3256331892473202}\n","     Tour:  {0}\n","  Edge (3, 13):\n","     Weight:  {0.5098507108265369}\n","     Tour:  {0}\n","  Edge (3, 14):\n","     Weight:  {0.10192800337320708}\n","     Tour:  {1}\n","  Edge (3, 15):\n","     Weight:  {0.4760316956166869}\n","     Tour:  {0}\n","  Edge (3, 16):\n","     Weight:  {0.2982239590456468}\n","     Tour:  {0}\n","  Edge (3, 17):\n","     Weight:  {0.4650265877604502}\n","     Tour:  {0}\n","  Edge (3, 18):\n","     Weight:  {0.40529952122582097}\n","     Tour:  {0}\n","  Edge (3, 19):\n","     Weight:  {0.3641842557466924}\n","     Tour:  {0}\n","  Edge (4, 5):\n","     Weight:  {0.5764321626357953}\n","     Tour:  {0}\n","  Edge (4, 6):\n","     Weight:  {0.30133829128093664}\n","     Tour:  {0}\n","  Edge (4, 7):\n","     Weight:  {0.5950103618814667}\n","     Tour:  {0}\n","  Edge (4, 8):\n","     Weight:  {0.5770449832745134}\n","     Tour:  {0}\n","  Edge (4, 9):\n","     Weight:  {0.2307340037832632}\n","     Tour:  {0}\n","  Edge (4, 10):\n","     Weight:  {0.3046349275334078}\n","     Tour:  {0}\n","  Edge (4, 11):\n","     Weight:  {0.26810007977040523}\n","     Tour:  {0}\n","  Edge (4, 12):\n","     Weight:  {0.3859450475320259}\n","     Tour:  {0}\n","  Edge (4, 13):\n","     Weight:  {0.5561153892125273}\n","     Tour:  {0}\n","  Edge (4, 14):\n","     Weight:  {0.24804828262824546}\n","     Tour:  {0}\n","  Edge (4, 15):\n","     Weight:  {0.41247528818868645}\n","     Tour:  {0}\n","  Edge (4, 16):\n","     Weight:  {0.410375847856154}\n","     Tour:  {0}\n","  Edge (4, 17):\n","     Weight:  {0.5656186289094377}\n","     Tour:  {0}\n","  Edge (4, 18):\n","     Weight:  {0.5483723633197157}\n","     Tour:  {0}\n","  Edge (4, 19):\n","     Weight:  {0.30210799803116456}\n","     Tour:  {0}\n","  Edge (5, 6):\n","     Weight:  {0.6867560124051679}\n","     Tour:  {0}\n","  Edge (5, 7):\n","     Weight:  {0.6992915692243714}\n","     Tour:  {0}\n","  Edge (5, 8):\n","     Weight:  {0.27073049472112015}\n","     Tour:  {0}\n","  Edge (5, 9):\n","     Weight:  {0.6078062084716456}\n","     Tour:  {0}\n","  Edge (5, 10):\n","     Weight:  {0.43958977527759663}\n","     Tour:  {0}\n","  Edge (5, 11):\n","     Weight:  {0.32199052710954257}\n","     Tour:  {1}\n","  Edge (5, 12):\n","     Weight:  {0.7727326567888985}\n","     Tour:  {0}\n","  Edge (5, 13):\n","     Weight:  {0.9501826066747174}\n","     Tour:  {0}\n","  Edge (5, 14):\n","     Weight:  {0.46911196324102344}\n","     Tour:  {0}\n","  Edge (5, 15):\n","     Weight:  {0.4935936115373682}\n","     Tour:  {0}\n","  Edge (5, 16):\n","     Weight:  {0.6639435819391838}\n","     Tour:  {0}\n","  Edge (5, 17):\n","     Weight:  {0.08842284642269631}\n","     Tour:  {1}\n","  Edge (5, 18):\n","     Weight:  {0.3771405718250126}\n","     Tour:  {0}\n","  Edge (5, 19):\n","     Weight:  {0.8562970048150594}\n","     Tour:  {0}\n","  Edge (6, 7):\n","     Weight:  {0.3452299871310383}\n","     Tour:  {0}\n","  Edge (6, 8):\n","     Weight:  {0.5566916991250165}\n","     Tour:  {0}\n","  Edge (6, 9):\n","     Weight:  {0.08818713565270636}\n","     Tour:  {1}\n","  Edge (6, 10):\n","     Weight:  {0.5806051325658653}\n","     Tour:  {0}\n","  Edge (6, 11):\n","     Weight:  {0.38358592867555963}\n","     Tour:  {0}\n","  Edge (6, 12):\n","     Weight:  {0.09446322848406911}\n","     Tour:  {0}\n","  Edge (6, 13):\n","     Weight:  {0.27914978407080576}\n","     Tour:  {0}\n","  Edge (6, 14):\n","     Weight:  {0.2178340373451969}\n","     Tour:  {0}\n","  Edge (6, 15):\n","     Weight:  {0.6973941311815144}\n","     Tour:  {0}\n","  Edge (6, 16):\n","     Weight:  {0.1437315155664361}\n","     Tour:  {0}\n","  Edge (6, 17):\n","     Weight:  {0.6372490513538306}\n","     Tour:  {0}\n","  Edge (6, 18):\n","     Weight:  {0.4701840720104889}\n","     Tour:  {0}\n","  Edge (6, 19):\n","     Weight:  {0.2774232612572184}\n","     Tour:  {1}\n","  Edge (7, 8):\n","     Weight:  {0.45952904895181307}\n","     Tour:  {0}\n","  Edge (7, 9):\n","     Weight:  {0.37417400645996296}\n","     Tour:  {0}\n","  Edge (7, 10):\n","     Weight:  {0.7992567869828283}\n","     Tour:  {0}\n","  Edge (7, 11):\n","     Weight:  {0.5148673440983987}\n","     Tour:  {0}\n","  Edge (7, 12):\n","     Weight:  {0.33571626255030856}\n","     Tour:  {0}\n","  Edge (7, 13):\n","     Weight:  {0.4026101124667492}\n","     Tour:  {0}\n","  Edge (7, 14):\n","     Weight:  {0.37340467136695177}\n","     Tour:  {0}\n","  Edge (7, 15):\n","     Weight:  {0.9174698538239648}\n","     Tour:  {0}\n","  Edge (7, 16):\n","     Weight:  {0.20177465871599579}\n","     Tour:  {1}\n","  Edge (7, 17):\n","     Weight:  {0.6195648253244966}\n","     Tour:  {0}\n","  Edge (7, 18):\n","     Weight:  {0.33917612945209147}\n","     Tour:  {1}\n","  Edge (7, 19):\n","     Weight:  {0.6162194325540812}\n","     Tour:  {0}\n","  Edge (8, 9):\n","     Weight:  {0.500606236365931}\n","     Tour:  {0}\n","  Edge (8, 10):\n","     Weight:  {0.5829012393087726}\n","     Tour:  {0}\n","  Edge (8, 11):\n","     Weight:  {0.3226682580193532}\n","     Tour:  {0}\n","  Edge (8, 12):\n","     Weight:  {0.6236465117971378}\n","     Tour:  {0}\n","  Edge (8, 13):\n","     Weight:  {0.7794753363995585}\n","     Tour:  {0}\n","  Edge (8, 14):\n","     Weight:  {0.3687755686215689}\n","     Tour:  {0}\n","  Edge (8, 15):\n","     Weight:  {0.6744208504349039}\n","     Tour:  {0}\n","  Edge (8, 16):\n","     Weight:  {0.48341874284217323}\n","     Tour:  {0}\n","  Edge (8, 17):\n","     Weight:  {0.1823906358666875}\n","     Tour:  {1}\n","  Edge (8, 18):\n","     Weight:  {0.12039747445267121}\n","     Tour:  {1}\n","  Edge (8, 19):\n","     Weight:  {0.7920456502968736}\n","     Tour:  {0}\n","  Edge (9, 10):\n","     Weight:  {0.4958159010103946}\n","     Tour:  {0}\n","  Edge (9, 11):\n","     Weight:  {0.2981604547970567}\n","     Tour:  {0}\n","  Edge (9, 12):\n","     Weight:  {0.18263885255286078}\n","     Tour:  {0}\n","  Edge (9, 13):\n","     Weight:  {0.3672998819800728}\n","     Tour:  {0}\n","  Edge (9, 14):\n","     Weight:  {0.14074192147940914}\n","     Tour:  {0}\n","  Edge (9, 15):\n","     Weight:  {0.6136048694347368}\n","     Tour:  {0}\n","  Edge (9, 16):\n","     Weight:  {0.1807984883622037}\n","     Tour:  {0}\n","  Edge (9, 17):\n","     Weight:  {0.5635976733439557}\n","     Tour:  {0}\n","  Edge (9, 18):\n","     Weight:  {0.4275922503419513}\n","     Tour:  {0}\n","  Edge (9, 19):\n","     Weight:  {0.2972159394392491}\n","     Tour:  {0}\n","  Edge (10, 11):\n","     Weight:  {0.29767923348870867}\n","     Tour:  {1}\n","  Edge (10, 12):\n","     Weight:  {0.6728443418750315}\n","     Tour:  {0}\n","  Edge (10, 13):\n","     Weight:  {0.8518446381557372}\n","     Tour:  {0}\n","  Edge (10, 14):\n","     Weight:  {0.42882230173921154}\n","     Tour:  {0}\n","  Edge (10, 15):\n","     Weight:  {0.11952991870231995}\n","     Tour:  {1}\n","  Edge (10, 16):\n","     Weight:  {0.6531686692237917}\n","     Tour:  {0}\n","  Edge (10, 17):\n","     Weight:  {0.4770670662090856}\n","     Tour:  {0}\n","  Edge (10, 18):\n","     Weight:  {0.6168640222169508}\n","     Tour:  {0}\n","  Edge (10, 19):\n","     Weight:  {0.5961301720109755}\n","     Tour:  {0}\n","  Edge (11, 12):\n","     Weight:  {0.47581620562438115}\n","     Tour:  {0}\n","  Edge (11, 13):\n","     Weight:  {0.6599029483947126}\n","     Tour:  {0}\n","  Edge (11, 14):\n","     Weight:  {0.17346768130254947}\n","     Tour:  {0}\n","  Edge (11, 15):\n","     Weight:  {0.41048539737715684}\n","     Tour:  {0}\n","  Edge (11, 16):\n","     Weight:  {0.4025683785940153}\n","     Tour:  {0}\n","  Edge (11, 17):\n","     Weight:  {0.29925677077431195}\n","     Tour:  {0}\n","  Edge (11, 18):\n","     Weight:  {0.3274947351745373}\n","     Tour:  {0}\n","  Edge (11, 19):\n","     Weight:  {0.5346097241040995}\n","     Tour:  {0}\n","  Edge (12, 13):\n","     Weight:  {0.18490797189553923}\n","     Tour:  {1}\n","  Edge (12, 14):\n","     Weight:  {0.3065099771993463}\n","     Tour:  {0}\n","  Edge (12, 15):\n","     Weight:  {0.7887404243083697}\n","     Tour:  {0}\n","  Edge (12, 16):\n","     Weight:  {0.15629069658405714}\n","     Tour:  {1}\n","  Edge (12, 17):\n","     Weight:  {0.7187869230155136}\n","     Tour:  {0}\n","  Edge (12, 18):\n","     Weight:  {0.5268902682284917}\n","     Tour:  {0}\n","  Edge (12, 19):\n","     Weight:  {0.28846075399036875}\n","     Tour:  {0}\n","  Edge (13, 14):\n","     Weight:  {0.48887714561053647}\n","     Tour:  {0}\n","  Edge (13, 15):\n","     Weight:  {0.9657246855588165}\n","     Tour:  {0}\n","  Edge (13, 16):\n","     Weight:  {0.2962429306808471}\n","     Tour:  {0}\n","  Edge (13, 17):\n","     Weight:  {0.8913669046263939}\n","     Tour:  {0}\n","  Edge (13, 18):\n","     Weight:  {0.6721854655144677}\n","     Tour:  {0}\n","  Edge (13, 19):\n","     Weight:  {0.3719877587298442}\n","     Tour:  {1}\n","  Edge (14, 15):\n","     Weight:  {0.5480609545208327}\n","     Tour:  {0}\n","  Edge (14, 16):\n","     Weight:  {0.2321206572622659}\n","     Tour:  {0}\n","  Edge (14, 17):\n","     Weight:  {0.42288927202931514}\n","     Tour:  {0}\n","  Edge (14, 18):\n","     Weight:  {0.3126013860373729}\n","     Tour:  {0}\n","  Edge (14, 19):\n","     Weight:  {0.4234389913230374}\n","     Tour:  {0}\n","  Edge (15, 16):\n","     Weight:  {0.7726413301352619}\n","     Tour:  {0}\n","  Edge (15, 17):\n","     Weight:  {0.5469248197512611}\n","     Tour:  {0}\n","  Edge (15, 18):\n","     Weight:  {0.7204237265004912}\n","     Tour:  {0}\n","  Edge (15, 19):\n","     Weight:  {0.6911317266141075}\n","     Tour:  {0}\n","  Edge (16, 17):\n","     Weight:  {0.599767667597168}\n","     Tour:  {0}\n","  Edge (16, 18):\n","     Weight:  {0.3787620993736398}\n","     Tour:  {0}\n","  Edge (16, 19):\n","     Weight:  {0.41838423486864734}\n","     Tour:  {0}\n","  Edge (17, 18):\n","     Weight:  {0.2912935061526579}\n","     Tour:  {0}\n","  Edge (17, 19):\n","     Weight:  {0.8291779696877807}\n","     Tour:  {0}\n","  Edge (18, 19):\n","     Weight:  {0.7246549985661137}\n","     Tour:  {0}\n"]}],"source":["# Describe the edge attributes tour and weight\n","\n","# Extract the graph and tour\n","graph = dummy[0][0]  # The networkx Graph object\n","tour = dummy[0][1]   # The tour as a list of nodes\n","\n","# Inspect edges with attributes\n","print(\"Edges and attributes:\")\n","for u, v, data in graph.edges(data=True):\n","    print(f\"  Edge ({u}, {v}):\")\n","    print('     Weight: ', {data.get('weight', 'Not found')})\n","    print('     Tour: ',{data.get('tour', 'Not found')})"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.282546Z","iopub.status.busy":"2024-12-27T15:02:14.282282Z","iopub.status.idle":"2024-12-27T15:02:14.292829Z","shell.execute_reply":"2024-12-27T15:02:14.291977Z","shell.execute_reply.started":"2024-12-27T15:02:14.282522Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Nodes and attributes:\n","  Node 0:\n","     Position:  {(0.6049077053425551, 0.5748590937018008)}\n","  Node 1:\n","     Position:  {(0.38474987528197846, 0.9428085200806016)}\n","  Node 2:\n","     Position:  {(0.6102491981278754, 0.3707527129445174)}\n","  Node 3:\n","     Position:  {(0.5497610140601452, 0.514788385568776)}\n","  Node 4:\n","     Position:  {(0.5941533303116413, 0.6544475361385552)}\n","  Node 5:\n","     Position:  {(0.06187381797691738, 0.433195284467101)}\n","  Node 6:\n","     Position:  {(0.7475717305758963, 0.3950876312718402)}\n","  Node 7:\n","     Position:  {(0.6548530739834322, 0.06254140180263457)}\n","  Node 8:\n","     Position:  {(0.2210796367473482, 0.2142238067774731)}\n","  Node 9:\n","     Position:  {(0.6696714621150585, 0.4364218673039507)}\n","  Node 10:\n","     Position:  {(0.3206284506117195, 0.7885615893113229)}\n","  Node 11:\n","     Position:  {(0.37760295594024584, 0.4963855605324464)}\n","  Node 12:\n","     Position:  {(0.8300002368321365, 0.3489482457969727)}\n","  Node 13:\n","     Position:  {(0.9983722301898076, 0.2725163812162502)}\n","  Node 14:\n","     Position:  {(0.5306200040273271, 0.4146737532387711)}\n","  Node 15:\n","     Position:  {(0.2540580284004623, 0.8878378722372976)}\n","  Node 16:\n","     Position:  {(0.7024553839343133, 0.2586205516051998)}\n","  Node 17:\n","     Position:  {(0.11129635562701035, 0.35987398782201374)}\n","  Node 18:\n","     Position:  {(0.33376928628552816, 0.1718375502602505)}\n","  Node 19:\n","     Position:  {(0.8952647398936215, 0.6299289412776933)}\n"]}],"source":["# Inspect the node attribute pos\n","print(\"\\nNodes and attributes:\")\n","for node, data in graph.nodes(data=True):\n","    print(f\"  Node {node}:\")\n","    print('     Position: ',{data.get('pos', 'Not found')})"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.293966Z","iopub.status.busy":"2024-12-27T15:02:14.293722Z","iopub.status.idle":"2024-12-27T15:02:14.304557Z","shell.execute_reply":"2024-12-27T15:02:14.303782Z","shell.execute_reply.started":"2024-12-27T15:02:14.293943Z"},"trusted":true},"outputs":[],"source":["# # Analyze the tour\n","# print(\"\\nTour edges:\")\n","# tour_edges = [(tour[i], tour[i+1]) for i in range(len(tour) - 1)]\n","# for u, v in tour_edges:\n","#     if graph.has_edge(u, v):\n","#         print(f\"   Edge ({u}, {v}) exists with weight {graph[u][v].get('weight', 'Not found')}.\")\n","#     else:\n","#         print(f\"   Edge ({u}, {v}) does not exist in the graph.\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.305546Z","iopub.status.busy":"2024-12-27T15:02:14.305338Z","iopub.status.idle":"2024-12-27T15:02:14.378488Z","shell.execute_reply":"2024-12-27T15:02:14.377593Z","shell.execute_reply.started":"2024-12-27T15:02:14.305525Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(0)\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.380087Z","iopub.status.busy":"2024-12-27T15:02:14.379723Z","iopub.status.idle":"2024-12-27T15:02:14.387313Z","shell.execute_reply":"2024-12-27T15:02:14.386494Z","shell.execute_reply.started":"2024-12-27T15:02:14.380048Z"},"trusted":true},"outputs":[],"source":["# Implement a dataset class. Focus on the getitem method to return:\n","# – X: A tensor of node coordinates with size 20 × 2.\n","# – y: A tour starting from 0 and ending with 0.\n","\n","class GraphDataset(Dataset):\n","    def __init__(self, data):\n","        \"\"\"\n","        A list of tuples where each tuple contains:\n","        - A networkx.Graph object\n","        - A tour (list of node indices)\n","        \"\"\"\n","        self.data = data\n","\n","    def __len__(self):\n","        \"\"\"\n","        Number of instances in the dataset\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns:\n","        - X: A tensor of node coordinates with size 20 × 2.\n","        - y: A tensor representing the tour, starting and ending at 0.\n","        \"\"\"\n","        # Extract the graph and tour\n","        graph, tour = self.data[idx]\n","\n","        # Get node positions as a 2D array\n","        pos = nx.get_node_attributes(graph, 'pos')  # Dictionary {node: (x, y)}\n","        if not pos:\n","            raise ValueError(f\"Graph at index {idx} is missing node positions ('pos').\")\n","\n","        # Ensure nodes are sorted by their index (important for consistent tensor order)\n","        sorted_positions = [pos[node] for node in sorted(graph.nodes())]\n","        \n","        # Convert positions to a tensor of shape (20, 2)\n","        X = torch.tensor(sorted_positions, dtype=torch.float32)\n","\n","        # Convert the tour to a tensor\n","        y = torch.tensor(tour, dtype=torch.long)\n","\n","        return X, y"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.388562Z","iopub.status.busy":"2024-12-27T15:02:14.388313Z","iopub.status.idle":"2024-12-27T15:02:14.425320Z","shell.execute_reply":"2024-12-27T15:02:14.424502Z","shell.execute_reply.started":"2024-12-27T15:02:14.388539Z"},"trusted":true},"outputs":[],"source":["# Create Dataset objects for training, validation, and testing, along with their respective Dataloader\n","dataset = GraphDataset(dummy)\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])\n","\n","batch_size = 32\n","\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.428874Z","iopub.status.busy":"2024-12-27T15:02:14.428589Z","iopub.status.idle":"2024-12-27T15:02:14.438582Z","shell.execute_reply":"2024-12-27T15:02:14.437677Z","shell.execute_reply.started":"2024-12-27T15:02:14.428850Z"},"trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self,emb_size: int, dropout: float, maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)   \n","        \n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","class TSPTransformer(nn.Module):\n","    def __init__(self, n, num_encoder, num_decoder, de, dd, n_head, dropout):\n","        super(TSPTransformer, self).__init__()\n","        # Encoder\n","        self.linear1 = nn.Linear(2, de)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=de, nhead=n_head) # d_model (int) – the number of expected features in the input (required).\n","        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder) # stack of n encoder layers\n","        self.linear2 = nn.Linear(de, dd)\n","        \n","        # Decoder\n","        self.embedding = nn.Embedding(n, dd)\n","        self.posEncoding = PositionalEncoding(dd, dropout)\n","        decoder_layer = nn.TransformerDecoderLayer(d_model=dd, nhead=n_head)\n","        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder)\n","        self.ffnn = nn.Linear(dd, n)\n","\n","    def forward(self,\n","            src: Tensor,\n","            trg: Tensor,\n","            src_mask: Tensor,\n","            tgt_mask: Tensor,\n","            src_padding_mask: Tensor,\n","            tgt_padding_mask: Tensor,\n","            memory_key_padding_mask: Tensor):\n","        # Encoding\n","        src = self.linear1(src)\n","        src = self.encoder(src, mask=src_mask, src_key_padding_mask=src_padding_mask)\n","        src = self.linear2(src)\n","        \n","        # Decoding\n","        trg = self.embedding(trg)\n","        trg = self.posEncoding(trg)\n","        output = self.decoder(trg, src, tgt_mask=tgt_mask, memory_mask=None,\n","                              tgt_key_padding_mask=tgt_padding_mask,\n","                              memory_key_padding_mask=memory_key_padding_mask)\n","        output = self.ffnn(output)\n","        return output"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.439957Z","iopub.status.busy":"2024-12-27T15:02:14.439624Z","iopub.status.idle":"2024-12-27T15:02:14.453025Z","shell.execute_reply":"2024-12-27T15:02:14.452297Z","shell.execute_reply.started":"2024-12-27T15:02:14.439923Z"},"trusted":true},"outputs":[],"source":["def generate_square_subsequent_mask(sz):\n","    ## Decoder mask\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[1]  \n","    tgt_seq_len = tgt.shape[1]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n","\n","    # Corrected padding mask dimensions (batch_size, sequence_length)\n","    src_padding_mask = torch.zeros((src.shape[0], src_seq_len), device=DEVICE).bool()  # Corrected\n","    tgt_padding_mask = torch.zeros((tgt.shape[0], tgt_seq_len), device=DEVICE).bool()  # Corrected\n","\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"]},{"cell_type":"markdown","metadata":{},"source":["$n$ = num di nodes -> Each TSP instance has exactly 20 nodes, and the input consists of their 2D coordinates.\n","\n","$de$ = size of the internal rapresentation of the input coordinates\n","       2 feature per node -> small values like 16, 32, 64\n","\n","$dd$ = size of intermediate representation\n","     y is a sequence of discrete node indeces, typical equal to de but also higher\n","\n","$Ne$ = num encoeer layers -> 2,4,6\n","\n","$Nd$ = num decoder layers -> equal to Ne but also higher"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.454347Z","iopub.status.busy":"2024-12-27T15:02:14.454053Z","iopub.status.idle":"2024-12-27T15:02:14.739108Z","shell.execute_reply":"2024-12-27T15:02:14.738326Z","shell.execute_reply.started":"2024-12-27T15:02:14.454316Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}],"source":["n = 20\n","n_enc = 4\n","n_dec = 4\n","de = 32\n","dd = 64\n","N_HEAD = 8\n","DROPOUT = 0.1\n","\n","TSPmodel = TSPTransformer(n, n_enc, n_dec, de, dd, N_HEAD, DROPOUT).to(DEVICE)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.740482Z","iopub.status.busy":"2024-12-27T15:02:14.740156Z","iopub.status.idle":"2024-12-27T15:02:14.746282Z","shell.execute_reply":"2024-12-27T15:02:14.745491Z","shell.execute_reply.started":"2024-12-27T15:02:14.740454Z"},"trusted":true},"outputs":[],"source":["# Function for training a single epoch\n","def train_epoch(model, optimizer, trainloader, loss_fn):\n","    model.train()\n","    losses = 0\n","\n","    for src, tgt in trainloader:\n","        src = src.to(DEVICE)  # Node coordinates (input to the encoder)\n","        tgt = tgt.to(DEVICE)  # Tour indices (target sequence for the decoder)\n","\n","        tgt_input = tgt[:, :-1]  # Input to the decoder (shifted by one token)\n","        tgt_out = tgt[:, 1:]  # Target for loss computation (shifted by one token)\n","\n","        optimizer.zero_grad()\n","\n","        # Generate masks for attention\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        # Forward pass\n","        output = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        # Update weights\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(trainloader)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.747754Z","iopub.status.busy":"2024-12-27T15:02:14.747403Z","iopub.status.idle":"2024-12-27T15:02:14.760214Z","shell.execute_reply":"2024-12-27T15:02:14.759459Z","shell.execute_reply.started":"2024-12-27T15:02:14.747719Z"},"trusted":true},"outputs":[],"source":["# Function for evaluation\n","def evaluate(model, valloader, loss_fn):\n","    model.eval()\n","    losses = 0\n","\n","    with torch.no_grad():\n","        for src, tgt in valloader:\n","            src = src.to(DEVICE)  # Node coordinates (input to the encoder)\n","            tgt = tgt.to(DEVICE)  # Tour indices (target sequence for the decoder)\n","          \n","            tgt_input = tgt[:, :-1]  # Input to the decoder (shifted by one token)\n","            tgt_out = tgt[:, 1:]  # Target for loss computation (shifted by one token)\n","\n","            # Generate masks for attention\n","            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","            # Forward pass\n","            output = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n","            losses += loss.item()\n","\n","    avg_loss = losses / len(valloader)\n","    return avg_loss"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-12-27T15:02:14.761494Z","iopub.status.busy":"2024-12-27T15:02:14.761223Z","iopub.status.idle":"2024-12-27T15:02:16.794170Z","shell.execute_reply":"2024-12-27T15:02:16.792853Z","shell.execute_reply.started":"2024-12-27T15:02:14.761466Z"},"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"The shape of the 2D attn_mask is torch.Size([20, 20]), but should be (32, 32).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTSPmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate on validation data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate(TSPmodel, valloader, loss_fn)\n","Cell \u001b[0;32mIn[15], line 19\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, trainloader, loss_fn)\u001b[0m\n\u001b[1;32m     16\u001b[0m src_mask, tgt_mask, src_padding_mask, tgt_padding_mask \u001b[38;5;241m=\u001b[39m create_mask(src, tgt_input)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), tgt_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[12], line 45\u001b[0m, in \u001b[0;36mTSPTransformer.forward\u001b[0;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     36\u001b[0m         src: Tensor,\n\u001b[1;32m     37\u001b[0m         trg: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         memory_key_padding_mask: Tensor):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(src)\n\u001b[0;32m---> 45\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(src)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Decoding\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:416\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    413\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 416\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    419\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:749\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    747\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    750\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:757\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    756\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 757\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5438\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5436\u001b[0m     correct_2d_size \u001b[38;5;241m=\u001b[39m (tgt_len, src_len)\n\u001b[1;32m   5437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m correct_2d_size:\n\u001b[0;32m-> 5438\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the 2D attn_mask is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_2d_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5439\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   5440\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n","\u001b[0;31mRuntimeError\u001b[0m: The shape of the 2D attn_mask is torch.Size([20, 20]), but should be (32, 32)."]}],"source":["warnings.filterwarnings(\"ignore\")\n","\n","# hyperparameters\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(TSPmodel.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)\n","\n","# Training configuration\n","NUM_EPOCHS = 10\n","training_losses = []\n","val_losses = []\n","\n","# Training loop\n","for epoch in range(1, NUM_EPOCHS + 1):\n","    start_time = timer()\n","    \n","    # Train for one epoch\n","    train_loss = train_epoch(TSPmodel, optimizer, trainloader, loss_fn)    \n","    \n","    # Evaluate on validation data\n","    val_loss = evaluate(TSPmodel, valloader, loss_fn)\n","    end_time = timer()\n","\n","    # Log losses\n","    training_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    \n","    print(f\"Epoch: {epoch}\")\n","    print(f\"Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Perplexity: {perplexity:.3f}\")\n","    print(f\"Epoch time = {(end_time - start_time):.3f}s\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-12-27T15:02:16.794998Z","iopub.status.idle":"2024-12-27T15:02:16.795322Z","shell.execute_reply":"2024-12-27T15:02:16.795192Z","shell.execute_reply.started":"2024-12-27T15:02:16.795176Z"},"trusted":true},"outputs":[],"source":["for src, tgt in valloader:\n","    src = src.to(DEVICE)  # Node coordinates (input to the encoder)\n","    tgt = tgt.to(DEVICE)  # Tour indices (target sequence for the decoder)\n","            \n","    print(f\"src shape: {src.shape}\")  # Should be (batch_size, seq_len, embedding_dim)\n","    print(f\"tgt shape: {tgt.shape}\")  # Should be (batch_size, seq_len, embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-12-27T15:02:16.797308Z","iopub.status.idle":"2024-12-27T15:02:16.797624Z","shell.execute_reply":"2024-12-27T15:02:16.797494Z","shell.execute_reply.started":"2024-12-27T15:02:16.797478Z"},"trusted":true},"outputs":[],"source":["tgt_input = tgt[:, :-1]  # Input to the decoder (shifted by one token)\n","src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","print(f\"src_mask shape: {src_mask.shape}\")\n","print(f\"src_padding_mask shape: {src_padding_mask.shape}\")\n","\n","src_mask = src_mask.repeat(batch_size, 1, 1)  # Repeat the mask for the batch size"]},{"cell_type":"markdown","metadata":{},"source":["### Training WITHOUT gradient accumulation"]},{"cell_type":"markdown","metadata":{},"source":["### Training WITH gradient accumulation"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6323224,"sourceId":10227445,"sourceType":"datasetVersion"}],"dockerImageVersionId":30804,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
